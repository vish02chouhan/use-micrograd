{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt # for making figures\n",
    "%matplotlib inline\n",
    "words = open('../../names.txt', 'r').read().splitlines()\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 200 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5) #* 0.2\n",
    "#W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * .2\n",
    "#b1 = torch.randn(n_hidden,                        generator=g) * 0.01\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.01\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0\n",
    "\n",
    "# BatchNorm parameters\n",
    "bngain = torch.ones((1, n_hidden))\n",
    "bnbias = torch.zeros((1, n_hidden))\n",
    "bnmean_running = torch.zeros((1, n_hidden))\n",
    "bnstd_running = torch.ones((1, n_hidden))\n",
    "\n",
    "parameters = [C, W1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 200000\n",
    "batch_size = 32\n",
    "lossi = []\n",
    "\n",
    "for i in range(10):\n",
    "  \n",
    "  # minibatch construct\n",
    "  ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "  Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y\n",
    "  \n",
    "  # forward pass\n",
    "  emb = C[Xb] # embed the characters into vectors\n",
    "  embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "  # Linear layer\n",
    "  hpreact = embcat @ W1  #+ b1 # hidden layer pre-activation\n",
    "  # BatchNorm layer\n",
    "  # -------------------------------------------------------------\n",
    "  bnmeani = hpreact.mean(0, keepdim=True)\n",
    "  bnstdi = hpreact.std(0, keepdim=True)\n",
    "  hpreact_n = bngain * (hpreact - bnmeani) / bnstdi + bnbias\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    bnmean_running = 0.999 * bnmean_running + 0.001 * bnmeani\n",
    "    bnstd_running = 0.999 * bnstd_running + 0.001 * bnstdi\n",
    "  # -------------------------------------------------------------\n",
    "  # Non-linearity\n",
    "  h = torch.tanh(hpreact_n) # hidden layer\n",
    "  logits = h @ W2 + b2 # output layer\n",
    "  loss = F.cross_entropy(logits, Yb) # loss function\n",
    "  \n",
    "  # backward pass\n",
    "  for p in parameters:\n",
    "    p.grad = None\n",
    "  loss.backward()\n",
    "  \n",
    "  # update\n",
    "  lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
    "  for p in parameters:\n",
    "    p.data += -lr * p.grad\n",
    "\n",
    "  # track stats\n",
    "  if i % 10000 == 0: # print every once in a while\n",
    "    print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
    "  lossi.append(loss.log10().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(hpreact[0]),min(hpreact[0]),max(hpreact1[0]),min(hpreact1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Your code to get hprev, h, logits, and probs\n",
    "# ...\n",
    "\n",
    "# Convert the tensors to NumPy and flatten them for histogram plotting\n",
    "hprev_numpy = hpreact.detach().numpy().flatten()\n",
    "hprev1_numpy = hpreact1.detach().numpy().flatten()\n",
    "h_mean = hpreact.mean(0, keepdim=True).detach().numpy().flatten()\n",
    "h_mean_sub = (hpreact - hpreact.mean(0, keepdim=True)).detach().numpy().flatten()\n",
    "\n",
    "# Create a figure with 2x2 grid of axes\n",
    "fig, axs = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Plot histogram for hprev\n",
    "axs[0, 0].hist(hprev_numpy, bins=50, color='blue', alpha=0.7)\n",
    "axs[0, 0].set_title('Histogram of hprev values')\n",
    "axs[0, 0].set_xlabel('Value')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "axs[0, 1].hist(hprev1_numpy, bins=50, color='blue', alpha=0.7)\n",
    "axs[0, 1].set_title('Histogram of hprev values')\n",
    "axs[0, 1].set_xlabel('Value')\n",
    "axs[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "axs[1, 0].hist(h_mean, bins=50, color='blue', alpha=0.7)\n",
    "axs[1, 0].set_title('Histogram of hmean values')\n",
    "axs[1, 0].set_xlabel('Value')\n",
    "axs[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "axs[1, 1].hist(h_mean_sub, bins=50, color='blue', alpha=0.7)\n",
    "axs[1, 1].set_title('Histogram of h_mean_sub values')\n",
    "axs[1, 1].set_xlabel('Value')\n",
    "axs[1, 1].set_ylabel('Frequency')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "16**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "9 = 3*3\n",
    "27 = 3*3*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "258\n",
    "1\n",
    "2*2\n",
    "\n",
    "\n",
    "8*8 = 64\n",
    "\n",
    "\n",
    "15*15 = 225\n",
    "\n",
    "16*16 = 256\n",
    "\n",
    "17 *17 = 349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(1/30)**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(5/3)/((30)**0.5),1/30**.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from helpers import graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "w1 = torch.randn(30,200) / 30**.5\n",
    "l1 = torch.nn.Linear(30,200) \n",
    "l1.weight.max(), w1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "w1 = torch.randn(150,200) / 150**.5\n",
    "l1 = torch.nn.Linear(150,200) \n",
    "l1.weight.max(), w1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = Linear(30,200, False)\n",
    "C = 27,10\n",
    "xb = 32, 3\n",
    "emb = c[xb] # 32,10,3\n",
    "embcat = 32,30\n",
    "l1(embcat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: \"../Batch-Normalization/10121-icon-service-Azure-Cosmos-DB.svg\" was not found as a file or as a shape library member\n",
      "Warning: No or improper image=\"../Batch-Normalization/10121-icon-service-Azure-Cosmos-DB.svg\" for node \"CosmosDB\"\n",
      "Warning: \"../Batch-Normalization/10121-icon-service-Azure-Cosmos-DB.svg\" was not found as a file or as a shape library member\n",
      "Warning: No or improper image=\"../Batch-Normalization/10121-icon-service-Azure-Cosmos-DB.svg\" for node \"AzureFunctions\"\n",
      "Warning: \"../Batch-Normalization/10121-icon-service-Azure-Cosmos-DB.svg\" was not found as a file or as a shape library member\n",
      "Warning: No or improper image=\"../Batch-Normalization/10121-icon-service-Azure-Cosmos-DB.svg\" for node \"MongoDB\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'..\\\\Batch-Normalization\\\\aa.svg'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "\n",
    "# Assuming you have the Azure icons downloaded and stored at the given paths\n",
    "cosmos_db_icon_path = '../Batch-Normalization/10121-icon-service-Azure-Cosmos-DB.svg'\n",
    "azure_functions_icon_path = '../Batch-Normalization/10121-icon-service-Azure-Cosmos-DB.svg'\n",
    "mongo_db_icon_path =  '../Batch-Normalization/10121-icon-service-Azure-Cosmos-DB.svg'\n",
    "\n",
    "# Create a Digraph object\n",
    "dot = Digraph('Architecture', format='svg')\n",
    "\n",
    "# Set graph attributes if necessary (e.g., size, rankdir)\n",
    "dot.attr(size='10,6')\n",
    "dot.attr(rankdir='LR')  # Left to Right, instead of Top to Bottom\n",
    "\n",
    "# Add nodes with the SVG images\n",
    "dot.node('CosmosDB', label='', image=cosmos_db_icon_path, shape='none')\n",
    "dot.node('AzureFunctions', label='', image=azure_functions_icon_path, shape='none')\n",
    "dot.node('MongoDB', label='', image=mongo_db_icon_path, shape='none')\n",
    "\n",
    "# Add edges between the nodes\n",
    "dot.edge('CosmosDB', 'AzureFunctions')\n",
    "dot.edge('AzureFunctions', 'MongoDB')\n",
    "\n",
    "# Save the diagram to a file\n",
    "dot.render('../Batch-Normalization/aa', format='svg', cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'G.gv.pdf'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "\n",
    "# Create a Digraph object\n",
    "dot = Digraph('G')\n",
    "\n",
    "# Add a node with an image\n",
    "dot.node('A', 'Node A', image='./output.png', shape='none')\n",
    "\n",
    "# Visualize the graph\n",
    "dot.view()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cairosvg\n",
    "cairosvg.svg2png(url='./dummy-svgrepo-com.svg', write_to='output.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn((10, 100), generator=g) / fan_in**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= [torch.tensor(2).float(),torch.tensor(5).float()]\n",
    "y = torch.tensor(5).float()\n",
    "\n",
    "w1 = torch.tensor(2.1).float()\n",
    "\n",
    "output = x*w1\n",
    "\n",
    "loss = (y - output)**2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, -0.9950547536867306, -0.9288576214547277, 0.0, 0.9950547536867305]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def custTanh(x):\n",
    "   return  (math.exp(3*x) - 1)/(math.exp(3*x) + 1)\n",
    "\n",
    "arr = [-17, -2,-1.1,0,2,]\n",
    "res = [custTanh(num) for num in arr] \n",
    "\n",
    "res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3, 4], [1, 2, 3, 4])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr = [1,2,3,4,5]\n",
    " \n",
    "arr[:4], arr[:-1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
