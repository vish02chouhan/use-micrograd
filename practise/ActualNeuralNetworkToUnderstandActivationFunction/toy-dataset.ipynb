{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Generate a toy dataset\n",
    "X, y = make_circles(n_samples=100, factor=0.5, noise=0.1, random_state=42)\n",
    "\n",
    "# Step 2: Train the Model With Activation Function (Sigmoid)\n",
    "clf_with_activation = MLPClassifier(activation='logistic', max_iter=1000, hidden_layer_sizes=(10,), random_state=42)\n",
    "clf_with_activation.fit(X, y)\n",
    "\n",
    "# Step 3: Train the Model Without Activation Function (Linear)\n",
    "clf_without_activation = MLPClassifier(activation='identity', max_iter=1000, hidden_layer_sizes=(10,), random_state=42)\n",
    "clf_without_activation.fit(X, y)\n",
    "\n",
    "# Step 4: Visualize the Decision Boundary\n",
    "def plot_decision_boundary(clf, X, y, ax, title):\n",
    "    # Plotting ranges\n",
    "    x_min, x_max = X[:, 0].min() - 0.1, X[:, 0].max() + 0.1\n",
    "    y_min, y_max = X[:, 1].min() - 0.1, X[:, 1].max() + 0.1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "    \n",
    "    # Predictions to obtain the decision boundary\n",
    "    Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plotting the decision boundary\n",
    "    ax.contourf(xx, yy, Z, alpha=0.8, levels=np.linspace(0, 1, 3))\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, s=20, edgecolor='k')\n",
    "    ax.set_title(title)\n",
    "\n",
    "# Create subplots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# Plot decision boundary with and without activation function\n",
    "plot_decision_boundary(clf_with_activation, X, y, ax[0], \"With Sigmoid Activation Function\")\n",
    "plot_decision_boundary(clf_without_activation, X, y, ax[1], \"Without Activation Function (Linear)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Example statements for training\n",
    "train_statements = [\n",
    "    \"I love sunny days\",\n",
    "    \"What a wonderful world\",\n",
    "    \"I hate being stuck in traffic\",\n",
    "    \"This is a terrible situation\",\n",
    "    \"I am very happy with the service\",\n",
    "    \"The food was bad\",\n",
    "    \"I am delighted to be part of the team\",\n",
    "    \"This is the worst movie I have ever seen\",\n",
    "    \"I am not satisfied with the product\",\n",
    "    \"He is my best friend\"\n",
    "]\n",
    "\n",
    "# Corresponding labels, 1 for positive and 0 for negative\n",
    "train_labels = [1, 1, 0, 0, 1, 0, 1, 0, 0, 1]\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_statements, train_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Creating a pipeline that first creates bag of words (after applying tokenization and stopwords removal) and then applies Naive Bayes classifier\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "# Training the model with the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions with the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "accuracy, report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Custom Dataset class for handling text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = self.build_vocab(self.texts)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_vocab(texts):\n",
    "        tokens = [word for text in texts for word in text.split()]\n",
    "        vocab = {word: i+1 for i, word in enumerate(set(tokens))}  # +1 for padding index\n",
    "        return vocab\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        return [self.vocab[word] for word in text.split()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded_text = self.encode_text(self.texts[index])\n",
    "        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        texts, labels = zip(*batch)\n",
    "        texts = pad_sequence(texts, batch_first=True, padding_value=0)  # Padding the sequences\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        return texts, labels\n",
    "\n",
    "# Simple neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, 10)\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = torch.sigmoid(x)  # Activation function\n",
    "        return x\n",
    "\n",
    "# Sample data and labels\n",
    "texts = [\"I love sunny days\", \"I hate rain\", \"This is great\", \"I am sad\", \"What a beautiful view\"]\n",
    "labels = [1, 0, 1, 0, 1]  # 1 for positive, 0 for negative\n",
    "\n",
    "# Train-test split\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# Creating datasets\n",
    "train_data = TextDataset(train_texts, train_labels)\n",
    "test_data = TextDataset(test_texts, test_labels)\n",
    "vocab_size = len(train_data.vocab) + 1  # +1 for padding index\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=TextDataset.collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=2, collate_fn=TextDataset.collate_fn)\n",
    "\n",
    "# Initialize the neural network\n",
    "model = SimpleNN(vocab_size)\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # Using Adam optimizer\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(5):  # Loop over the dataset multiple times\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Testing the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Inference mode, no gradients\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs).squeeze()\n",
    "        predicted = outputs.round()  # Threshold predictions to get binary classification\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, vocab, texts):\n",
    "    # Tokenize and encode the new text\n",
    "    tokens = [[vocab.get(word, 0) for word in text.split()] for text in texts]\n",
    "    lengths = [len(token) for token in tokens]\n",
    "\n",
    "    # Pad the sequences\n",
    "    padded_tokens = torch.zeros(len(tokens), max(lengths)).long()\n",
    "    for i, token in enumerate(tokens):\n",
    "        padded_tokens[i, :lengths[i]] = torch.tensor(token)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(padded_tokens).squeeze()\n",
    "    \n",
    "    # Debugging prints\n",
    "    print(\"Encoded texts:\", tokens)\n",
    "    print(\"Model raw outputs:\", outputs)\n",
    "    \n",
    "    predictions = outputs.round().numpy()  # Convert to numpy array\n",
    "    \n",
    "    # Convert predictions to text labels\n",
    "    labels = ['Positive' if pred == 1 else 'Negative' for pred in predictions]\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "new_texts = [\"This product is really good\", \"I'm not happy with this service\"]\n",
    "predictions = predict_sentiment(model, train_data.vocab, new_texts)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assume you have downloaded the Sentiment140 dataset\n",
    "# and it is in a CSV file called 'sentiment140.csv'\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../../data-sets/sentiment140.csv', encoding='latin1', usecols=[0, 5], names=['sentiment', 'text'])\n",
    "\n",
    "# Preprocess the tweets\n",
    "# ... here you would add your preprocessing steps, like removing URLs, Twitter handles, etc.\n",
    "\n",
    "# Encode the sentiments (0 for negative, 1 for positive)\n",
    "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
    "\n",
    "# Split the dataset\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Proceed with creating your TextDataset instances and DataLoaders\n",
    "train_data = TextDataset(train_df['text'].tolist(), train_df['sentiment'].tolist())\n",
    "train_data = torch.utils.data.Subset(train_data, range(1000))\n",
    "test_data = TextDataset(test_df['text'].tolist(), test_df['sentiment'].tolist())\n",
    "train_data = torch.utils.data.Subset(train_data, range(1000, 2000))\n",
    "\n",
    "# The rest of the code for training and evaluation follows...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
