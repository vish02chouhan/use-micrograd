{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Custom Dataset class for handling text\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.vocab = self.build_vocab(self.texts)\n",
    "\n",
    "    @staticmethod\n",
    "    def build_vocab(texts):\n",
    "        tokens = [word for text in texts for word in text.split()]\n",
    "        vocab = {word: i+1 for i, word in enumerate(set(tokens))}  # +1 for padding index\n",
    "        return vocab\n",
    "    \n",
    "    def encode_text(self, text):\n",
    "        return [self.vocab[word] for word in text.split()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        encoded_text = self.encode_text(self.texts[index])\n",
    "        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(self.labels[index], dtype=torch.float32)\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        texts, labels = zip(*batch)\n",
    "        texts = pad_sequence(texts, batch_first=True, padding_value=0)  # Padding the sequences\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        return texts, labels\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=10, hidden_dim=10):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # An embedding layer that converts input data (indices of words) into embeddings\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # A fully connected layer that maps embeddings to hidden_dim space\n",
    "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # Output layer that maps from hidden space to 1 output (for binary classification)\n",
    "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Pass data through the embedding layer\n",
    "        # The input x should be of shape (batch_size, sequence_length)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        # Take the mean of the embeddings (an example of simple pooling)\n",
    "        pooled = F.avg_pool2d(embedded, (embedded.shape[1], 1)).squeeze(1)\n",
    "        \n",
    "        # Pass the pooled embeddings through the fully connected layer with ReLU activation\n",
    "        hidden = F.relu(self.fc1(pooled))\n",
    "        \n",
    "        # Pass the result through the output layer and apply the sigmoid activation function\n",
    "        # The output will be a batch of single numbers (probabilities)\n",
    "        output = torch.sigmoid(self.fc2(hidden))\n",
    "        \n",
    "        return output.squeeze()  # Squeeze to remove any extra dimensions if output is for single example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume you have downloaded the Sentiment140 dataset\n",
    "# and it is in a CSV file called 'sentiment140.csv'\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../../data-sets/sentiment140.csv', encoding='latin1', usecols=[0, 5], names=['sentiment', 'text'])\n",
    "\n",
    "# Preprocess the tweets\n",
    "# ... here you would add your preprocessing steps, like removing URLs, Twitter handles, etc.\n",
    "\n",
    "# Encode the sentiments (0 for negative, 1 for positive)\n",
    "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
    "\n",
    "df1 = df[df[\"sentiment\"] == 1][0:5000]\n",
    "df2 = df[df[\"sentiment\"] == 0][0:5000]\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Split the dataset\n",
    "train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Proceed with creating your TextDataset instances and DataLoaders\n",
    "train_data = TextDataset(train_df['text'].tolist(), train_df['sentiment'].tolist())\n",
    "test_data = TextDataset(test_df['text'].tolist(), test_df['sentiment'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>800000</th>\n",
       "      <td>1</td>\n",
       "      <td>I LOVE @Health4UandPets u guys r the best!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800001</th>\n",
       "      <td>1</td>\n",
       "      <td>im meeting up with one of my besties tonight! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800002</th>\n",
       "      <td>1</td>\n",
       "      <td>@DaRealSunisaKim Thanks for the Twitter add, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800003</th>\n",
       "      <td>1</td>\n",
       "      <td>Being sick can be really cheap when it hurts t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>800004</th>\n",
       "      <td>1</td>\n",
       "      <td>@LovesBrooklyn2 he has that effect on everyone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804995</th>\n",
       "      <td>1</td>\n",
       "      <td>@lbran, thanks for sending us the package - go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804996</th>\n",
       "      <td>1</td>\n",
       "      <td>@ickleoriental hahahha.. U obviously don't hv ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804997</th>\n",
       "      <td>1</td>\n",
       "      <td>@juliekoh It's an internet term, but it's spil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804998</th>\n",
       "      <td>1</td>\n",
       "      <td>new day.... NEW TRACK!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804999</th>\n",
       "      <td>1</td>\n",
       "      <td>@foodieguide Okay we need to have a competitio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        sentiment                                               text\n",
       "800000          1       I LOVE @Health4UandPets u guys r the best!! \n",
       "800001          1  im meeting up with one of my besties tonight! ...\n",
       "800002          1  @DaRealSunisaKim Thanks for the Twitter add, S...\n",
       "800003          1  Being sick can be really cheap when it hurts t...\n",
       "800004          1    @LovesBrooklyn2 he has that effect on everyone \n",
       "...           ...                                                ...\n",
       "804995          1  @lbran, thanks for sending us the package - go...\n",
       "804996          1  @ickleoriental hahahha.. U obviously don't hv ...\n",
       "804997          1  @juliekoh It's an internet term, but it's spil...\n",
       "804998          1                         new day.... NEW TRACK!!!! \n",
       "804999          1  @foodieguide Okay we need to have a competitio...\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.502\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(train_data.vocab) + 1# +1 for padding index\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=2, shuffle=True, collate_fn=TextDataset.collate_fn)\n",
    "test_loader = DataLoader(test_data, batch_size=2, collate_fn=TextDataset.collate_fn)\n",
    "\n",
    "# Initialize the neural network\n",
    "model = SimpleNN(vocab_size)\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = Adam(model.parameters(), lr=0.001)  # Using Adam optimizer\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(5):  # Loop over the dataset multiple times\n",
    "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Testing the model\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # Inference mode, no gradients\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs).squeeze()\n",
    "        predicted = outputs.round()  # Threshold predictions to get binary classification\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded texts: [[10356, 19682, 1894, 16613, 21461, 1983], [7004, 12282, 24033, 12737, 19106, 16153, 12429]]\n",
      "Model raw outputs: tensor([0.9169, 0.2497])\n",
      "['Positive', 'Negative']\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(model, vocab, texts):\n",
    "    # Tokenize and encode the new text\n",
    "    tokens = [[vocab.get(word, 0) for word in text.split()] for text in texts]\n",
    "    lengths = [len(token) for token in tokens]\n",
    "\n",
    "    # Pad the sequences\n",
    "    padded_tokens = torch.zeros(len(tokens), max(lengths)).long()\n",
    "    for i, token in enumerate(tokens):\n",
    "        padded_tokens[i, :lengths[i]] = torch.tensor(token)\n",
    "    \n",
    "    # Make predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(padded_tokens).squeeze()\n",
    "    \n",
    "    # Debugging prints\n",
    "    print(\"Encoded texts:\", tokens)\n",
    "    print(\"Model raw outputs:\", outputs)\n",
    "    \n",
    "    predictions = outputs.round().numpy()  # Convert to numpy array\n",
    "    \n",
    "    # Convert predictions to text labels\n",
    "    labels = ['Positive' if pred == 1 else 'Negative' for pred in predictions]\n",
    "    return labels\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "new_texts = [\"This product is really very good\", \"I'm not happy with this bad service\"]\n",
    "predictions = predict_sentiment(model, train_data.vocab, new_texts)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for lab in train_data.labels if lab == 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
