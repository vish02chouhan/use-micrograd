{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "data = \".abcd\"\n",
    "\n",
    "stoi = { char:i for i,char in enumerate(data)}\n",
    "itos = { i:char for i,char in enumerate(data)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "#example = \"c\"\n",
    "#. -> c\n",
    "xenc = F.one_hot(torch.tensor([0,3]), num_classes=5).float()\n",
    "print(xenc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-0.1034 -> -0.8899\n",
    "h = .5\n",
    "W = torch.tensor(\n",
    "    [\n",
    "\n",
    "        [ 0.2963 , 1.1951, 0.7433 , -0.1034, -0.6192],\n",
    "        [ 2.6439, 1.7843, 0.2086, 0.5000, 0.4406], \n",
    "        [-0.6386, 0.2266, -0.0906, -0.3895, -0.2717],\n",
    "        [-0.2562, -0.7472, -0.8306, -1.2820, 0.2468],\n",
    "        [-1.3791, -1.8291, -0.4827, 0.2922, 0.4291]\n",
    "\n",
    "]\n",
    "    , requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3704e-04,  9.5608e-04,  5.9464e-04, -8.2720e-05, -4.9536e-04],\n",
      "        [ 2.1151e-03,  1.4274e-03,  1.6688e-04,  4.0000e-04,  3.5248e-04],\n",
      "        [-5.1088e-04,  1.8128e-04, -7.2480e-05, -3.1160e-04, -2.1736e-04],\n",
      "        [-2.0496e-04, -5.9776e-04, -6.6448e-04, -1.0256e-03,  1.9744e-04],\n",
      "        [-1.1033e-03, -1.4633e-03, -3.8616e-04,  2.3376e-04,  3.4328e-04]])\n"
     ]
    }
   ],
   "source": [
    "reg = .01 * (W**2).mean()\n",
    "\n",
    "W.grad = None\n",
    "reg.backward()\n",
    "print(W.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1153,  0.1820,  0.0986,  2.1218, -0.7810],\n",
      "        [ 2.6333,  1.7772,  0.2078,  0.4980,  0.4388],\n",
      "        [-0.6360,  0.2257, -0.0902, -0.3879, -0.2706],\n",
      "        [-0.8522,  1.3904, -1.1634, -1.4909, -0.7415],\n",
      "        [-1.3736, -1.8218, -0.4808,  0.2910,  0.4274]])\n",
      "tensor([[ 8.2324e-02,  2.0262e-01,  1.2895e-01, -4.4504e-01,  3.2365e-02],\n",
      "        [ 2.1151e-03,  1.4274e-03,  1.6688e-04,  4.0000e-04,  3.5248e-04],\n",
      "        [-5.1088e-04,  1.8128e-04, -7.2480e-05, -3.1160e-04, -2.1736e-04],\n",
      "        [ 1.1921e-01, -4.2752e-01,  6.6569e-02,  4.1784e-02,  1.9766e-01],\n",
      "        [-1.1033e-03, -1.4633e-03, -3.8616e-04,  2.3376e-04,  3.4328e-04]])\n"
     ]
    }
   ],
   "source": [
    "for i  in range(1):\n",
    "    \n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True) \n",
    "    loss = -probs[[0,1], [3, 1]].log().mean() + .01 * (W**2).mean()\n",
    "\n",
    "    #current scenerio probability multiplication-> - log(liklihood) is equals to loss\n",
    "    \n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    W.data += -5 * W.grad\n",
    "\n",
    "print(W.data)\n",
    "print(W.grad.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          [ 0.2963 , 1.1951, 0.7433 , -0.1034, -0.6192],\n",
    "       NC [ 2.6439, 1.7843, 0.2086, 0.5000, 0.4406], \n",
    "       NC [-0.6386, 0.2266, -0.0906, -0.3895, -0.2717],\n",
    "          [-0.2562, -0.7472, -0.8306, -1.2820, 0.2468],\n",
    "       NC [-1.3791, -1.8291, -0.4827, 0.2922, 0.4291]\n",
    "\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "        [-1.0960, -0.9979, -1.0235,  6.0266, -1.3971],\n",
    "        [ 2.6439,  1.7843,  0.2086,  0.5000,  0.4406],\n",
    "        [-0.6386,  0.2266, -0.0906, -0.3895, -0.2717],\n",
    "        [-1.9169,  5.1455, -2.0336, -2.1833, -1.8809],\n",
    "        [-1.3791, -1.8291, -0.4827,  0.2922,  0.4291]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum([-8.2720e-05, -0.4450])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "formatted_number = \"{:.10f}\".format(-4.4504e-01)\n",
    "print(formatted_number)\n",
    "formatted_number = \"{:.10f}\".format(-8.2720e-05)\n",
    "print(formatted_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{-4.4504e-01}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xenc = F.one_hot(torch.tensor([0]), num_classes=5).float()\n",
    "print(xenc)\n",
    "logits = xenc @ W\n",
    "print(logits)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True) \n",
    "print(probs) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
