
0:00
hi everyone today we are continuing our implementation of makemore now in the last lecture we implemented the
0:05
multi-layer perceptron along the lines of benjiotyle 2003 for character level language modeling so we followed this
0:11
paper took in a few characters in the past and used an MLP to predict the next character in a sequence
0:17
so what we'd like to do now is we'd like to move on to more complex and larger neural networks like recurrent neural
0:22
networks and their variations like the Gru lstm and so on now before we do that though we have to
0:28
stick around the level of multilia perceptron for a bit longer and I'd like to do this because I would like us to
0:33
have a very good intuitive understanding of the activations in the neural net during training and especially the
0:39
gradients that are flowing backwards and how they behave and what they look like and this is going to be very important
0:44
to understand the history of the development of these architectures because we'll see that recurrent neural networks while they are very expressive
0:51
in that they are a universal approximator and can in principle Implement uh all the algorithms we'll
0:58
see that they are not very easily optimizable with the first order ingredient-based techniques that we have available to us and that we use all the
1:03
time and the key to understanding why they are not optimizable easily is to
1:09
understand the the activations and the gradients and how they behave during training and we'll see that a lot of the variants since recurring neural networks
1:16
have tried to improve that situation and so that's the path that we have to take
1:21
and let's get started so the starting code for this lecture is largely the code from four but I've cleaned it up a
starter code
1:27
little bit so you'll see that we are importing all the torch and matplotlab utilities we're
1:33
reading in the words just like before these are eight example words there's a total of 32 000 of them here's a
1:39
vocabulary of all the lowercase letters and the special dot token here we are reading in the data set and
1:46
processing it and creating three splits the train Dev and the test split
1:53
now in MLP this is the identical same MLP except you see that I removed a bunch of magic numbers that we had here
1:59
and instead we have the dimensionality of the embedding space of the characters and the number of hidden units in the
2:05
hidden layer and so I've pulled them outside here so that we don't have to go and change all these magic numbers all
2:10
the time with the same neural net with 11 000 parameters that we optimize now over 200
2:16
000 steps with a batch size of 32 and you'll see that I refactor I refactored the code here a little bit but there are
2:22
no functional changes I just created a few extra variables a few more comments and I removed all the magic numbers and
2:29
otherwise is the exact same thing then when we optimize we saw that our loss looked something like this we saw
2:36
that the train and Val loss were about 2.16 and so on
2:41
here I refactored the code a little bit for the evaluation of arbitrary splits
2:46
so you pass in a string of which split you'd like to evaluate and then here depending on train Val or test I index
2:53
in and I get the correct split and then this is the forward pass of the network and evaluation of the loss and printing
2:59
it so just making it nicer one thing that you'll notice here is
3:05
I'm using a decorator torch.nograd which you can also look up and read the
3:10
documentation of basically what this decorator does on top of a function is that whatever happens in this function
3:17
is assumed by torch to never require any gradients so it will not do any of the
3:23
bookkeeping that it does to keep track of all the gradients in anticipation of an eventual backward pass
3:29
it's almost as if all the tensors that get created here have a requires grad of false and so it just makes everything
3:35
much more efficient because you're telling torch that I will not call that backward on any of this computation and
3:40
you don't need to maintain the graph under the hood so that's what this does and you can
3:46
also use a context manager with torch.nograd and you can look those up
3:52
then here we have the sampling from a model um just as before just a poor passive and neural net getting the distribution
3:59
sampling from it adjusting the context window and repeating until we get the special and token and we see that we are
4:05
starting to get much nicer looking words sample from the model but still not amazing and they're still not fully
4:12
named like but it's much better than when we had them with the byground model
4:17
so that's our starting point now the first thing I would like to scrutinize is the initialization I can tell that our network is very
fixing the initial loss
4:25
improperly configured at initialization and there's multiple things wrong with it but let's just start with the first
4:30
one look here on the zeroth iteration the very first iteration we are recording a loss of 27 and this
4:37
rapidly comes down to roughly one or two or so I can tell that the initialization is all messed up because this is way too
4:43
high in training of neural Nets it is almost always the case that you will have a rough idea for what loss to expect at
4:49
initialization and that just depends on the loss function and the problem set up in this case I do not expect 27. I
4:57
expect a much lower number and we can calculate it together basically at initialization what we'd
5:02
like is that there's 27 characters that could come next for any one training
5:08
example at initialization we have no reason to believe any characterist to be much more likely than others and so we'd
5:14
expect that the probability distribution that comes out initially is a uniform distribution assigning about equal
5:20
probability to all the 27 characters so basically what we like is the
5:25
probability for any character would be roughly 1 over 27.
5:31
that is the probability we should record and then the loss is the negative log probability so let's wrap this in a
5:37
tensor and then that one can take the log of it and then the negative log probability is
5:44
the loss we would expect which is 3.29 much much lower than 27.
5:49
and so what's happening right now is that at initialization the neural net is creating probability distributions that
5:55
are all messed up some characters are very confident and some characters are very not confident
6:00
and then basically what's happening is that the network is very confidently wrong and uh that make that's what makes it um
6:08
record very high loss so here's a smaller four-dimensional example of the issue let's say we only have four
6:14
characters and then we have Logics that come out of the neural land and they are very very close to zero
6:20
then when we take the soft Max of all zeros we get probabilities there are a diffuse distribution
6:27
so sums to one and is exactly uniform and then in this case if the label is
6:33
say 2 it doesn't actually matter if this if the label is 2 or 3 or 1 or 0 because
6:38
it's a uniform distribution we're recording the exact same loss in this case 1.38 so this is the loss we would
6:44
expect for a four-dimensional example and I can see of course that as we start to manipulate these logits we're going
6:50
to be changing the loss here so it could be that we lock out and by chance this
6:56
could be a very high number like you know five or something like that then in that case we'll record a very low loss
7:01
because we're assigning the correct probability at the initialization by chance to the correct label
7:06
much more likely it is that some other dimension will have a high uh logit and
7:14
then what will happen is we start to record much higher loss and what can come what can happen is basically The Lodges come out like something like this
7:21
you know and they take on Extreme values and we record really high loss
7:27
um for example if we have torch.randen of four so these are uniform sorry these
7:34
are normally distributed numbers uh forum
7:40
and here we can also print the logits the probabilities that come out of it and the loss and so because these logits
7:48
are near zero for the most part the loss that comes out is is okay but suppose this is like times 10 now
7:58
you see how because these are more extreme values it's very unlikely that you're going to be guessing the correct
8:04
bucket and then you're confidently wrong and recording very high loss if your lodges are coming out even more
8:11
extreme you might get extremely insane losses like infinity even at initialization
8:18
um so basically this is not good and we want the load just to be roughly zero
8:24
um when the network is initialized in fact the logits can don't have to be just zero they just have to be equal so
8:31
for example if all the objects are one then because of the normalization inside the softmax this will actually come out
8:37
okay but by symmetry we don't want it to be any arbitrary positive or negative number we just want it to be all zeros
8:43
and record the loss that we expect at initialization so let's Now quickly see where things go wrong in our example
8:49
here we have the initialization let me reinitialize the neural net and here let me break after the very first
8:55
iteration so we only see the initial loss which is 27. so that's way too high and intuitively
9:02
now we can expect the variables involved and we see that the logits here if we just print some of these
9:09
if we just print the first row we see that the load just take on quite extreme values and that's what's creating the fake
9:15
confidence and incorrect answers and makes the loss get very very high so these Lotus should
9:23
be much much closer to zero so now let's think through how we can achieve logits
9:28
coming out of this neural net to be more closer to zero you see here that lodges are calculated
9:34
as the hidden States multiplied by W2 plus B2 so first of all currently we're
9:39
initializing B2 as random values of the right size but because we want roughly zero we
9:46
don't actually want to be adding a bias of random numbers so in fact I'm going to add a times a zero here to make sure
9:52
that B2 is just basically zero at the initialization and second this is H multiplied by W2 so
10:00
if we want logits to be very very small then we would be multiplying W2 and making that smaller
10:06
so for example if we scale down W2 by 0.1 all the elements then if I
10:12
do again just the very first iteration you see that we are getting much closer to what we expect so the rough roughly
10:18
what we want is about 3.29 this is 4.2 I can make this maybe even smaller
10:25
3.32 okay so we're getting closer and closer now you're probably wondering can
10:31
we just set this to zero then we get of course exactly what we're looking for at initialization
10:37
and the reason I don't usually do this is because I'm I'm very nervous and I'll show you in a second why you don't want
10:44
to be setting W's or weights of a neural net exactly to zero um you usually want
10:50
it to be small numbers instead of exactly zero for this output layer in this specific case I think it would be
10:56
fine but I'll show you in a second where things go wrong very quickly if you do that so let's just go with 0.01
11:02
in that case our loss is close enough but has some entropy it's not exactly zero it's got some low entropy and
11:10
that's used for symmetry breaking as we'll see in a second the logits are now coming out much closer to zero and everything is well
11:17
and good so if I just erase these and I now take
11:22
away the break statement we can run the optimization with this new initialization and let's just see
11:29
what losses we record okay so I let it run and you see that we started off good
11:35
and then we came down a bit the plot of the loss now doesn't have
11:40
this hockey shape appearance um because basically what's happening in the hockey stick the very first few
11:47
iterations of the loss what's happening during the optimization is the optimization is just squashing down the
11:52
logits and then it's rearranging the logits so basically we took away this easy part of the loss function where
11:59
just the the weights were just being shrunk down and so therefore we don't we don't get
12:04
these easy gains in the beginning and we're just getting some of the hard gains of training the actual neural nut and so there's no hockey stick
12:10
appearance so good things are happening in that both number one lawsuit initialization
12:15
is what we expect and the the loss doesn't look like a hockey stick and this is true for any
12:22
neuron that you might train and something to look out for and second the last that came out is
12:28
actually quite a bit improved unfortunately I erased what we had here before I believe this was
12:33
2.12 and this is what this was 2.16 so we get a slightly improved result and
12:40
the reason for that is uh because we're spending more Cycles more time optimizing the neural net actually
12:45
instead of just spending the first several thousand iterations probably just squashing down the weights
12:53
because they are so way too high in the beginning in the initialization so something to look out for and uh
fixing the saturated tanh
12:59
that's number one now let's look at the second problem let me re-initialize our neural net and let me reintroduce the
13:04
break statement so we have a reasonable initial loss so even though everything is looking good
13:09
on the level of the loss and we get something that we expect there's still a deeper problem working inside this
13:15
neural net and its initialization so the logits are now okay the problem
13:20
now is with the values of H the activations of the Hidden States now
13:25
if we just visualize this Vector sorry this tensor H it's kind of hard to see but the problem here roughly speaking is
13:31
you see how many of the elements are one or negative one now recall that torch.nh the 10h
13:38
function is a squashing function it takes arbitrary numbers and it squashes them into a range of negative one and
13:44
one and it does so smoothly so let's look at the histogram of H to get a better idea of the distribution of
13:50
the values inside this tensor we can do this first well we can see that H is 32 examples
13:57
and 200 activations in each example we can view it as negative one to stretch
14:03
it out into one large vector and we can then call to list to convert
14:09
this into one large python list of floats and then we can pass this into plt.hist
14:16
for histogram and we say we want 50 bins and a semicolon to suppress a bunch of
14:22
output we don't want so we see this histogram and we see that most the values by far take on the value
14:28
of negative one and one so this 10h is very very active and we can also look at
14:35
basically why that is we can look at the pre-activations that feed into the 10h
14:42
and we can see that the distribution of the pre-activations are is very very broad these take numbers between
14:48
negative 15 and 15 and that's why in the torture 10h everything is being squashed and capped to be in the range of
14:54
negative one and one and lots of numbers here take on very extreme values now if you are new to neural networks
15:01
you might not actually see this as an issue but if you're well burst in the dark arts back propagation and then have
15:06
an intuitive sense of how these gradients flow through a neural net you are looking at your distribution of 10h
15:12
activations here and you are sweating so let me show you why we have to keep in mind that during that propagation
15:18
just like we saw in micrograd we are doing backward pass starting at the loss and flowing through the network
15:23
backwards in particular we're going to back propagate through this tors.10h
15:28
and this layer here is made up of 200 neurons for each one of these examples
15:33
and it implements an element twice 10 H so let's look at what happens in 10h in the backward pass
15:39
we can actually go back to our previous micrograd code in the very first lecture and see how we implement the 10h
15:46
we saw that the input here was X and then we calculate T which is the 10h of x
15:52
so that's T and T is between negative 1 and 1. it's the output of the 10h and then in the backward pass how do we back
15:58
propagate through a 10 h we take out that grad and then we multiply it this is the
16:04
chain rule with the local gradient which took the form of 1 minus t squared so what happens if the outputs of your
16:10
10h are very close to negative one or one if you plug in t equals one here you're going to get a zero multiplying
16:18
out that grad no matter what up.grad is we are killing the gradient and we're
16:23
stopping effectively the back propagation through this 10h unit similarly when T is negative one this
16:29
will again become zero and out that grad just stops and intuitively this makes sense because
16:34
this is a 10 H neuron and what's happening is if its output is
16:40
very close to one then we are in the tail of this 10 h and so changing basically the input
16:48
is not going to impact the output of the 10h too much because it's it's so it's in a flat region of the 10h and so
16:56
therefore there's no impact on the loss and so indeed the the weights and the
17:02
biases along with this tan H neuron do not impact the loss because the output of the standard unit is in the flat
17:07
region in the 10h and there's no influence we can we can be changing them whatever we want however we want and the
17:13
loss is not impacted that's that's another way to justify that indeed the gradient would be basically zero it
17:19
vanishes indeed when T equals zero we get 1 times at that grad so when the
17:28
10h takes on exactly value of zero then out.grad is just passed through
17:34
so basically what this is doing right is if T is equal to zero then this the 10h unit is uh sort of inactive
17:42
and uh gradient just passes through but the more you are in the flat tails the
17:47
more the gradient is squashed so in fact you'll see that the the gradient flowing through 10 H can only
17:53
ever decrease in the amount that it decreases is proportional through a
17:59
square here depending on how far you are in the flat tails of this 10 age
18:04
and so that's kind of what's Happening Here and through this the concern here
18:10
is that if all of these outputs H are in the flat regions of negative one and one then the gradients that are flowing
18:16
through the network will just get destroyed at this layer now there is some redeeming quality here
18:23
and that we can actually get a sense of the problem here as follows I've wrote some code here and basically
18:29
what we want to do here is we want to take a look at H take the absolute value and see how often it is in the in a flat
18:37
region so say greater than 0.99 and what you get is the following and
18:44
this is a Boolean tensor so uh in the Boolean tensor you get a white if this is true and a black and this is false
18:52
and so basically what we have here is the 32 examples and then 200 hidden neurons and we see that a lot of this is
18:59
white and what that's telling us is that all these 10h neurons were very very
19:05
active and uh they're in a flat tail and so in all these cases uh the back
19:12
the backward gradient would get destroyed now we would be in a lot of trouble if
19:19
for ever for any one of these 200 neurons if it was the case that the entire column is white because in that
19:26
case we have what's called a dead neuron and this could be a tannage neuron where the initialization of the weights and the biases could be such that no single
19:33
example ever activates this 10h in the sort of active part of the 10h if all
19:40
the examples land in the tail then this neuron will never learn it is a dead
19:45
neuron and so just scrutinizing this and looking for Columns of completely white
19:51
we see that this is not the case so I don't see a single neuron that is all of
19:57
uh you know white and so therefore it is the case that for every one of these 10h neurons
20:03
we do have some examples that activate them in the active part of the 10h and
20:09
so some gradients will flow through and this neuron will learn and the neuron will change and it will move and it will
20:14
do something but you can sometimes get yourself in cases where you have dead neurons and
20:20
the way this manifests is that for 10 inch neuron this would be when no matter what inputs you plug in from your data
20:27
set this 10 inch neuron always fires completely one or completely negative one and then it will just not learn
20:32
because all the gradients will be just zeroed out uh this is true not just percentage but
20:38
for a lot of other non-linearities that people use in neural networks so we certainly use 10h a lot but sigmoid will
20:44
have the exact same issue because it is a squashing neuron and so the same will be true for sigmoid but
20:51
um but um you know um basically the same will actually applied to sigmoid the same will also
20:57
reply to a relu so relu has a completely flat region here below zero
21:03
so if you have a relative neuron then it is a pass-through um if it is positive and if it's if the
21:10
pre-activation is negative it will just shut it off since the region here is completely flat then during back
21:15
propagation uh this would be exactly zeroing out the gradient um like all of the gradient would be set
21:22
exactly to zero instead of just like a very very small number depending on how positive or negative T is
21:28
and so you can get for example a dead relu neuron and a dead relinuron would
21:33
basically look like basically what it is is if a neuron with a relu nonlinearity never activates
21:41
so for any examples that you plug in in the data set it never turns on it's always in this flat region then this
21:47
reli neuron is a dead neuron it's weights and bias will never learn they will never get a gradient because the
21:53
neuron never activated and this can sometimes happen at initialization because the weights and
21:58
the biases just make it so that by chance some neurons are just forever dead but it can also happen during
22:03
optimization if you have like a too high of the learning rate for example sometimes you have these neurons that
22:09
gets too much of a gradient and they get knocked out off the data manifold and what happens is that from then on no
22:16
example ever activates this neuron so this neuron remains that forever so it's kind of like a permanent brain damage in
22:21
a in a mind of a network and so sometimes what can happen is if your learning rate is very high for
22:26
example and you have a neural net with regular neurons you train the neural net and you get some last loss but then
22:33
actually what you do is you go through the entire training set and you forward your examples and you can find neurons
22:40
that never activate they are dead neurons in your network and so those neurons will will never turn on and
22:46
usually what happens is that during training these relative neurons are changing moving Etc and then because of a high gradient somewhere by chance they
22:53
get knocked off and then nothing ever activates them and from then on they are just dead
22:58
uh so that's kind of like a permanent brain damage that can happen to some of these neurons these other nonlinearities like leaky
23:05
relu will not suffer from this issue as much because you can see that it doesn't have flat tails you almost always get
23:11
gradients and elu is also fairly frequently used it also might suffer from this issue
23:18
because it has flat parts so that's just something to be aware of and something to be concerned about and
23:24
in this case we have way too many um activations H that take on Extreme
23:29
values and because there's no column of white I think we will be okay and indeed
23:34
the network optimizes and gives us a pretty decent loss but it's just not optimal and this is not something you
23:40
want especially during initialization and so basically what's happening is that this H pre-activation that's
23:46
flowing to 10h it's it's too extreme it's too large it's creating very
23:53
um it's creating a distribution that is too saturated in both sides of the 10h and it's not something you want because
23:58
it means that there's less training uh for these neurons because they update
24:04
um less frequently so how do we fix this well HP activation is
24:09
MCAT which comes from C so these are uniform gaussian but then it's
24:15
multiplied by W1 plus B1 and H preact is too far off from zero and that's causing
24:20
the issue so we want this reactivation to be closer to zero very similar to what we have with lodges
24:27
so here we want actually something very very similar now it's okay to set the biases to very
24:34
small number we can either multiply it by zero zero one to get like a little bit of entropy um I sometimes like to do that
24:41
um just so that there's like a little bit of variation and diversity in the original
24:47
initialization of these 10h neurons and I find in practice that that can help optimization a little bit
24:53
and then the weights we can also just like squash so let's multiply everything by 0.1
24:59
let's rerun the first batch and now let's look at this and well
25:04
first let's look here you see now because we multiply doubly by 0.1 we have a much better histogram
25:10
and that's because the pre-activations are now between negative 1.5 and 1.5 and this we expect much much less white
25:18
okay there's no white so basically that's because there are no neurons that's saturated above 0.99 in
25:26
either direction this is actually a pretty decent place to be um maybe we can go up a little bit
25:36
it's very much am I changing W1 here so maybe we can go to point two
25:42
okay so maybe something like this is is a nice distribution so maybe this is
25:47
what our initialization should be so let me now erase these
25:53
and let me starting with initialization let me run the full optimization without
25:59
the break and uh let's see what we got okay so the optimization finished and I
26:05
Rebrand the loss and this is the result that we get and then just as a reminder I put down all the losses that we saw
26:10
previously in this lecture so we see that we actually do get an improvement here and just as a reminder
26:16
we started off with a validation loss of 2.17 when we started by fixing the softmax being confidently wrong we came
26:22
down to 2.13 and by fixing the 10h layer being way too saturated we came down to 2.10
26:28
and the reason this is happening of course is because our initialization is better and so we're spending more time doing productive training instead of
26:35
um not very productive training because our gradients are set to zero and we have to
26:41
learn very simple things like the overconfidence of the softmax in the beginning and we're spending Cycles just
26:46
like squashing down the weight Matrix so this is illustrating
26:51
um basically initialization and its impacts on performance just by being aware of the internals of these neural
26:58
Nets and their activations their gradients now we're working with a very small Network this is just one layer
27:03
multiplayer perceptron so because the network is so shallow the optimization problem is actually quite easy and very
27:10
forgiving so even though our initialization was terrible the network still learned eventually it just got a
27:16
bit worse result this is not the case in general though once we actually start working with much deeper networks that
27:23
have say 50 layers things can get much more complicated and these problems
27:28
Stack Up and so you can actually get into a place where the network is basically not
27:34
training at all if your initialization is bad enough and the deeper your network is and the more complex it is
27:39
the less forgiving it is to some of these errors and so um something that we definitely be aware
27:45
of and uh something to scrutinize something to plot and something to be careful with and um
27:52
yeah okay so that's great that that worked for us but what we have here now is all these metric numbers like point
calculating the init scale: “Kaiming init”
27:58
two like where do I come up with this and how am I supposed to set these if I have a large neural left with lots and
28:03
lots of layers and so obviously no one does this by hand there's actually some relatively
28:08
principled ways of setting these scales um that I would like to introduce to you now
28:13
so let me paste some code here that I prepared just to motivate the discussion of this
28:19
so what I'm doing here is we have some random input here x that is drawn from a gaussian and there's 1000 examples that
28:27
are 10 dimensional and then we have a weight and layer here that is also initialized using gaussian just like we
28:33
did here and we these neurons in the hidden layer look at 10 inputs and there are 200
28:39
neurons in this hidden layer and then we have here just like here um in this case the multiplication X
28:46
multiplied by W to get the pre-activations of these neurons and basically the analysis here looks at
28:52
okay suppose these are uniform gaussian and these weights are uniform gaussian if I do x times W and we forget for now
29:00
the bias and the non-linearity then what is the mean and the standard deviation of these gaussians
29:06
so in the beginning here the input is uh just a normal gaussian distribution mean zero and the standard deviation is one
29:13
and the standard deviation again is just a measure of a spread of discussion
29:18
but then once we multiply here and we look at the histogram of Y we see that
29:24
the mean of course stays the same it's about zero because this is a symmetric operation but we see here that the
29:29
standard deviation has expanded to three so the input standard deviation was one but now we've grown to three and so what
29:36
you're seeing in the histogram is that this gaussian is expanding and so
29:42
um we're expanding this gaussian from the input and we don't want that we want most of the neural Nets to have
29:48
relatively similar activations so unit gaussian roughly throughout the neural net and so the question is how do we
29:55
scale these wfs to preserve the um to preserve this distribution to remain a
30:02
gaussian and so intuitively if I multiply here uh these elements of w by a large number
30:09
let's say by five then this gaussian grows and grows in
30:14
standard deviation so now we're at 15. so basically these numbers here in the output y take on more and more extreme
30:21
values but if we scale it down well I say 0.2 then conversely this gaussian is getting
30:27
smaller and smaller and it's shrinking and you can see that the standard deviation is 0.6 and so the question is
30:34
what do I multiply by here to exactly preserve the standard deviation to be one
30:40
and it turns out that the correct answer mathematically when you work out through the variance of this multiplication here
30:47
is that you are supposed to divide by the square root of the fan in the fan in
30:53
is the basically the uh number of input elements here 10. so we are supposed to
30:58
divide by 10 square root and this is one way to do the square root you raise it to a power of 0.5 that's the same as
31:05
doing a square root so when you divide by the square root of 10 then we see that
31:12
the output gaussian it has exactly standard deviation of one now
31:17
unsurprisingly a number of papers have looked into how but to best initialize neural networks and in the case of
31:24
multiplayer perceptions we can have fairly deep networks that have these nonlinearities in between and we want to
31:29
make sure that the activations are well behaved and they don't expand to infinity or Shrink all the way to zero
31:35
and the question is how do we initialize the weights so that these activations take on reasonable values throughout the network
31:40
now one paper that has stuck this in quite a bit of detail that is often referenced is this paper by coming here
31:46
at all called the delving deep into rectifiers now in this case they actually study convolutional neural
31:52
networks and they studied especially the relu nonlinearity and the p-valued
31:57
nonlinearity instead of a 10 H nonlinearity but the analysis is very similar and
32:02
um basically what happens here is for them the the relation that they care
32:08
about quite a bit here is a squashing function where all the negative numbers are simply clamped to zero so the
32:16
positive numbers are passed through but everything negative is just set to zero and because uh you are basically
32:22
throwing away half of the distribution they find in their analysis of the forward activations in the neural net
32:28
that you have to compensate for that with a gain and so here
32:33
they find that basically when they initialize their weights they have to do it with a zero mean gaussian whose
32:39
standard deviation is square root of 2 over the fan in what we have here is we are initializing
32:45
a concussion with the square root of fanin this NL here is the Fanon so what we
32:52
have is square root of one over the fan in because we have the division here
32:57
now they have to add this factor of 2 because of the relu which basically discards half of the distribution and
33:04
clamps it at zero and so that's where you get an initial Factor now in addition to that this paper also
33:10
studies not just the uh sort of behavior of the activations in the forward pass of the neural net but it also studies
33:16
the back propagation and we have to make sure that the gradients also are well behaved and so
33:22
um because ultimately they end up updating our parameters and what they find here through a lot of
33:27
the analysis that I invite you to read through but it's not exactly approachable what they find is basically
33:33
if you properly initialize the forward pass the backward pass is also approximately initialized up to a
33:40
constant factor that has to do with the size of the number of hidden neurons in
33:45
an early and uh late layer and uh but basically they find
33:50
empirically that this is not a choice that matters too much now this timing initialization is also
33:56
implemented in pytorch so if you go to torch.nn.net documentation you'll find timing normal
34:02
and in my opinion this is probably the most common way of initializing neural networks now and it takes a few keyword arguments
34:09
here so number one it wants to know the mode would you like to normalize the
34:14
activations or would you like to normalize the gradients to to be always gaussian with zero mean and a unit or
34:20
one standard deviation and because they find the paper that this doesn't matter too much most of the people just leave
34:26
it as the default which is Fan in and then second passing the nonlinearity that you are using because depending on
34:32
the nonlinearity we need to calculate a slightly different gain and so if your nonlinearity is just linear so there's
34:39
no nonlinearity then the gain here will be one and we have the exact same uh kind of formula that we've got here
34:46
but if the nonlinearity is something else we're going to get a slightly different gain and so if we come up here to the top
34:52
we see that for example in the case of relu this gain is a square root of 2. and the reason it's a square root
34:57
because in this paper
35:02
you see how the two is inside of the square root so the gain is a square root
35:07
of 2. in the case of linear or identity we just get a gain of one in the case of
35:14
10h which is what we're using here the advised gain is a 5 over 3. and intuitively why do we need a gain on
35:21
top of the initialization is because 10h just like relu is a contractive transformation so what that means is
35:28
you're taking the output distribution from this matrix multiplication and then you are squashing it in some way now
35:33
relu squashes it by taking everything below zero and clamping it to zero tan H also squashes it because it's a
35:39
contractual operation it will take the Tails and it will squeeze them in and so in order to fight
35:45
the squeezing in we need to boost the weights a little bit so that we renormalize everything back to standard
35:51
unit standard deviation so that's why there's a little bit of a gain that comes out now I'm skipping through this section A
35:58
little bit quickly and I'm doing that actually intentionally and the reason for that is because about seven years ago when this paper
36:04
was written you had to actually be extremely careful with the activations and ingredients and their ranges and
36:10
their histograms and you have to be very careful with the precise setting of gains and the scrutinizing of the nonlinearities used and so on and
36:17
everything was very finicky and very fragile and very properly arranged for the neural not to train especially if
36:23
your neural network was very deep but there are a number of modern innovations that have made everything significantly more stable and more
36:28
well-behaved and has become less important to initialize these networks exactly right and some of those modern Innovations for
36:35
example are residual connections which we will cover in the future the use of a number of normalization layers like for
36:43
example batch normalization layer normalization group normalization we're going to go into a lot of these as well
36:48
and number three much better optimizers not just stochastic gradient descent the simple Optimizer we're basically using
36:54
here but a slightly more complex optimizers like RMS prop and especially Adam and so all of these modern
37:00
Innovations make it less important for you to precisely calibrate the initialization of the neural net all
37:06
that being said in practice uh what should we do in practice when I initialize these neural Nets I basically
37:12
just normalize my weights by the square root of the fan in uh so basically uh
37:18
roughly what we did here is what I do now if we want to be exactly accurate here we and go by init of coming normal
37:27
this is how a good implemented we want to set the standard deviation to be gained over the square root of fan n
37:33
right so to set the standard deviation of our weights we will proceed as
37:39
follows basically when we have a torch that renin and let's say I just create a
37:45
thousand numbers we can look at the standard deviation of this and of course that's one that's the amount of spread let's make this a bit bigger so it's
37:51
closer to one so that's the spread of the gaussian of zero mean and unit standard deviation
37:58
now basically when you take these and you multiply by say 0.2 that basically scales down the gaussian
38:04
and that makes its standard deviation 0.2 so basically the number that you multiply by here ends up being the
38:09
standard deviation of this caution so here this is a standard deviation 0.2
38:15
gaussian here when we sample rw1 but we want to set the standard deviation to gain over square root of
38:23
fan mode which is valid so in other words we want to multiply by gain which for 10 H is 5 over 3.
38:34
5 over 3 is the gain and then times
38:39
um I guess sorry divide
38:47
uh square root of the fan in and in this example here the fan in was 10 and I
38:54
just noticed that actually here the fan in for W1 is actually an embed times block size which as you all recall is
39:00
actually 30 and that's because each character is 10 dimensional but then we have three of them and we concatenate them so actually the fan in here was 30
39:07
and I should have used 30 here probably but basically we want 30 square root so
39:13
this is the number this is what our standard deviation we want to be and this number turns out to be 0.3
39:19
whereas here just by fiddling with it and looking at the distribution and making sure it looks okay we came up
39:24
with 0.2 and so instead what we want to do here is we want to make the standard deviation B
39:31
um 5 over 3 which is our gain divide
39:36
this amount times 0.2 square root and these brackets
39:42
here are not that necessary but I'll just put them here for clarity this is basically what we want this is the
39:48
chiming in it in our case for a 10h nonlinearity and this is how we would
39:53
initialize the neural net and so we're multiplying by 0.3 instead of
39:58
multiplying by 0.2 and so we can we can initialize this way and then we
40:05
can train the neural net and see what we got okay so I trained the neural net and we end up in roughly the same spot so
40:12
looking at the validation loss we now get 2.10 and previously we also had 2.10 and there's a little bit of a difference
40:18
but that's just the randomness of the process I suspect but the big deal of course is we get to the same spot but we did not have to
40:25
introduce any magic numbers that we got from just looking at histograms and
40:31
guessing checking we have something that is semi-principled and will scale us to much bigger networks and uh something
40:38
that we can sort of use as a guide so I mentioned that the precise setting of these initializations is not as
batch normalization
40:43
important today due to some Modern Innovations and I think now is a pretty good time to introduce one of those modern Innovations and that is best
40:49
normalization so batch normalization came out in 2015 from a team at Google and it was an
40:56
extremely impactful paper because it made it possible to train very deep neural Nets quite reliably and uh it
41:03
basically just worked so here's what nationalization does and what's implemented um
41:09
basically we have these hidden States HP act right and we were talking about how
41:14
we don't want these uh these pre-activation states to be way too
41:20
small because then the 10h is not doing anything but we don't want them to be too large because then the 10h is
41:26
saturated in fact we want them to be roughly roughly gaussian so zero mean and a unit
41:31
or one standard deviation at least at initialization so the Insight from The Bachelor
41:38
normalization paper is okay you have these hidden States and you'd like them to be roughly gaussian then why not take
41:44
the hidden States and just normalize them to be gaussian and it sounds kind
41:49
of crazy but you can just do that because uh standardizing hidden States
41:55
so that their unit caution is a perfectly differentiable operation as we'll soon see and so that was kind of
42:00
like the big Insight in this paper and when I first read it my mind was blown because you can just normalize these
42:05
hidden States and if you'd like unit gaussian States in your network at least initialization you can just normalize
42:12
them to be in gaussian so let's see how that works so we're going to scroll to
42:17
our pre-activations here just before they enter into the 10 age now the idea again is remember we're
42:22
trying to make these roughly gaussian and that's because if these are way too small numbers then the 10h here is kind
42:29
of connective but if these are very large numbers then the 10h is way too
42:34
saturated and grade is in the flow so we'd like this to be roughly caution so the Insight in bathroomization again
42:41
is that we can just standardize these activations so they are exactly gaussian
42:46
so here hpact has a shape of 32 by 200 32 examples by
42:53
200 neurons in the hidden layer so basically what we can do is we can take hpact and we can just calculate the
42:59
mean and the mean we want to calculate across the zeroth dimension
43:05
and we want to also keep them as true so that we can easily broadcast this
43:11
so the shape of this is 1 by 200 in other words we are doing the mean over all the uh elements in the
43:19
batch and similarly we can calculate the standard deviation of these activations
43:26
and that will also be one by 200. now in this paper they have the
43:32
uh sort of prescription here and see here we are calculating the mean which
43:37
is just taking the average value of any neurons activation and then the
43:44
standard deviation is basically kind of like this the measure of the spread that we've been using which is the distance
43:51
of every one of these values away from the mean and that squared and averaged
43:58
that's the that's the variance and then if you want to take the standard deviation you would square root the
44:04
variance to get the standard deviation so these are the two that we're calculating and now we're going to
44:10
normalize or standardize these X's by subtracting the mean and um dividing by the standard deviation
44:17
so basically we're taking Edge preact and we subtract
44:23
the mean
44:29
and then we divide by the standard deviation this is exactly what these two STD and
44:36
mean are calculating oops sorry this is the mean and this is the
44:42
variance you see how the sigma is the standard deviation usually so this is Sigma Square which is variance is the
44:47
square of the standard deviation so this is how you standardize these values and what this will do is that
44:54
every single neuron now and its firing rate will be exactly unit gaussian on these 32 examples at least of this batch
45:01
that's why it's called batch normalization we are normalizing these batches and then we could in principle train
45:09
this notice that calculating the mean and the standard deviation these are just mathematical formulas they're perfectly differentiable all this is
45:15
perfectly differentiable and we can just strain this the problem is you actually won't achieve a very good result with this and
45:23
the reason for that is we want these to be roughly gaussian but only at initialization but we don't want
45:30
these to be to be forced to be gaussian always we would actually We'll add the neural nuts to move this around to
45:37
potentially make it more diffuse to make it more sharp to make some 10 H neurons maybe mean more trigger more trigger
45:43
happy or less trigger happy so we'd like this distribution to move around and we'd like the back propagation to tell
45:49
us how that distribution should move around and so in addition to this idea
45:54
of standardizing the activations at any point in the network uh we have to also introduce this
46:00
additional component in the paper here describe the scale and shift and so basically what we're doing is
46:07
we're taking these normalized inputs and we are additionally scaling them by some gain and offsetting them by some bias to
46:14
get our final output from this layer and so what that amounts to is the following
46:20
we are going to allow a batch normalization gain to be initialized at just a once
46:27
and the ones will be in the shape of 1 by n hidden and then we also will have a b and bias
46:35
which will be torched at zeros and it will also be of the shape n by 1
46:40
by and hidden and then here the B and gain will multiply this
46:47
and the BN bias will offset it here so because this is initialized to one
46:52
and this to zero at initialization each neuron's firing
46:58
values in this batch will be exactly unit gaussian and we'll have nice numbers no matter what the distribution
47:04
of the hpact is coming in coming out it will be in gaussian for each neuron and
47:09
that's roughly what we want at least at initialization um and then during optimization we'll be
47:15
able to back propagate to be in game and being biased and change them so the network is given the full ability to do
47:22
with this whatever it wants uh internally here we just have to make sure that we
47:28
um include these in the parameters of the neural nut because they will be trained with back propagation
47:35
so let's initialize this and then we should be able to train
47:45
and then we're going to also copy this line which is the best normalization layer
47:51
here on a single line of code and we're going to swing down here and we're also going to do the exact same thing at test
47:57
time here so similar to training time we're going
48:03
to normalize and then scale and that's going to give us our train and validation loss
48:10
and we'll see in a second that we're actually going to change this a little bit but for now I'm going to keep it this way so I'm just going to wait for this to
48:16
converge okay so I'll add the neural nuts to converge here and when we scroll down we see that our validation loss
48:22
here is 2.10 roughly which I wrote down here and we see that this is actually
48:27
kind of comparable to some of the results that we've achieved previously now I'm not actually expecting an
48:33
improvement in this case and that's because we are dealing with a very simple neural nut that has just a single hidden layer so in fact in this very
48:41
simple case of just one hidden layer we were able to actually calculate what the scale of w should be to make these
48:47
pre-activations already have a roughly gaussian shape so the best normalization is not doing much here
48:53
but you might imagine that once you have a much deeper neural nut that has lots of different types of operations and
48:59
there's also for example residual connections which we'll cover and so on it will become basically very very
49:04
difficult to tune those scales of your weight matrices such that all the activations throughout the neural Nets
49:11
are roughly gaussian and so that's going to become very quickly intractable but compared to that
49:17
it's going to be much much easier to sprinkle batch normalization layers throughout the neural net so in particular it's common to to look
49:24
at every single linear layer like this one this is a linear layer multiplying by a weight Matrix and adding the bias
49:30
or for example convolutions which we'll cover later and also perform basically a multiplication with the weight Matrix
49:37
but in a more spatially structured format it's custom it's customary to take these linear layer or convolutional
49:43
layer and append a bachelorization layer right after it to control the scale of
49:49
these activations at every point in the neural net so we'd be adding these bathroom layers throughout the neural net and then this controls the scale of
49:56
these activations throughout the neural net it doesn't require us to do a perfect mathematics and care about the
50:02
activation distributions for all these different types of neural network Lego building blocks that you might want to
50:07
introduce into your neural net and it significantly stabilizes uh the training and that's why these layers are quite
50:14
popular now the stability offered by batch normalization actually comes at a terrible cost and that cost is that if
50:20
you think about what's Happening Here something something terribly strange and unnatural is happening
50:26
it used to be that we have a single example feeding into a neural net and then we calculate this activations and
50:33
it's logits and this is a deterministic sort of process so you arrive at some
50:38
Logics for this example and then because of efficiency of training we suddenly started to use batches of examples but
50:45
those batches of examples were processed independently and it was just an efficiency thing but now suddenly in bash normalization
50:51
because of the normalization through the batch we are coupling these examples mathematically and in the forward pass
50:57
and the backward pass of the neural land so now the hidden State activations hpact and your logits for any one input
51:04
example are not just a function of that example and its input but they're also a function of all the other examples that
51:11
happen to come for a ride in that batch and these examples are sampled randomly and so what's happening is for example
51:17
when you look at each preact that's going to feed into H the hidden State activations for for example for for any
51:23
one of these input examples is going to actually change slightly depending on what other examples there are in a batch
51:30
and and depending on what other examples happen to come for a ride H is going to change subtly and it's
51:36
going to like Jitter if you imagine sampling different examples because the statistics of the mean and the standard
51:41
deviation are going to be impacted and so you'll get a Jitter for H and you'll get a Jitter for logits
51:48
and you think that this would be a bug or something undesirable but in a very strange way this actually turns out to
51:55
be good in neural network training and as a side effect and the reason for that is that you can think of this as kind of
52:02
like a regularizer because what's happening is you have your input and you get your age and then depending on the
52:07
other examples this is generating a bit and so what that does is that it's effectively padding out any one of these
52:13
input examples and it's introducing a little bit of entropy and um because of the padding out it's
52:18
actually kind of like a form of data augmentation which we'll cover in the future and it's kind of like augmenting
52:24
the input a little bit and it's jittering it and that makes it harder for the neural nuts to overfit to these
52:30
concrete specific examples so by introducing all this noise it actually like Pats out the examples and it
52:36
regularizes the neural net and that's one of the reasons why deceivingly as a second order effect
52:41
this is actually a regularizer and that has made it harder for us to remove the
52:46
use of batch normalization because basically no one likes this property that the the examples in the
52:52
batch are coupled mathematically and in the forward pass and at least all kinds of like strange results uh we'll go into
52:59
some of that in a second as well um and it leads to a lot of bugs and um and so on and so no one likes this
53:05
property uh and so people have tried to deprecate the use of astronomization and
53:11
move to other normalization techniques that do not couple the examples of a batch examples are layer normalization
53:16
instance normalization group normalization and so on and we'll cover we'll cover some of these later
53:23
um but basically long story short bash formalization was the first kind of normalization layer to be introduced it
53:29
worked extremely well it happens to have this regularizing effect it stabilized training
53:35
and people have been trying to remove it and move to some of the other normalization techniques but it's been
53:41
hard because it just works quite well and some of the reason that it works quite well is again because of this
53:47
regularizing effect and because of the because it is quite effective at controlling the activations and their
53:53
distributions uh so that's kind of like the brief story of nationalization and I'd like to
53:58
show you one of the other weird sort of outcomes of this coupling
54:03
so here's one of the strange outcomes that I only glossed over previously when I was evaluating the loss on the
54:09
validation side basically once we've trained a neural net we'd like to deploy it in some kind
54:15
of a setting and we'd like to be able to feed in a single individual example and get a prediction out from our neural net
54:21
but how do we do that when our neural net now in a forward pass estimates the statistics of the mean energy standard
54:26
deviation of a batch the neural net expects badges as an input now so how do we feed in a single example and get
54:32
sensible results out and so the proposal in the batch normalization paper is the following
54:38
what we would like to do here is we would like to basically have a step after training that calculates and sets
54:47
the bathroom mean and standard deviation a single time over the training set
54:52
and so I wrote this code here in interest of time and we're going to call what's called calibrate the Bachelor of
54:57
statistics and basically what we do is not no grad telling pytorch that none of this we
55:04
will call the dot backward on and it's going to be a bit more efficient we're going to take the training set get
55:11
the pre-activations for every single training example and then one single time estimate the mean and standard deviation over the entire training set
55:18
and then we're going to get B and mean and be in standard deviation and now these are fixed numbers as the meaning
55:23
of the entire training set and here instead of estimating it dynamically
55:29
we are going to instead here use B and mean and here we're just going to use B and
55:36
standard deviation and so at this time we are going to fix these clamp them and use them during
55:42
inference and now you see that we get basically identical
55:47
result but the benefit that we've gained is that we can now also forward a single
55:52
example because the mean and standard deviation are now fixed uh sort of tensors that said nobody actually wants to
55:59
estimate this mean and standard deviation as a second stage after neural network training because everyone is
56:05
lazy and so this batch normalization paper actually introduced one more idea which is that we can we can estimate the
56:11
mean and standard deviation in a running matter running manner during training of the neural net and then we can simply
56:18
just have a single stage of training and on the side of that training we are estimating the running mean and standard
56:23
deviation so let's see what that would look like let me basically take the mean here that
56:28
we are estimating on the batch and let me call this B and mean on the I iteration
56:34
um and then here this is B and sdd
56:40
um bnstd at I okay
56:46
uh and the mean comes here and the STD
56:51
comes here so so far I've done nothing I've just moved around and I created these extra variables for the mean and
56:57
standard deviation and I've put them here so so far nothing has changed but what we're going to do now is we're
57:03
going to keep a running mean of both of these values during training so let me swing up here and let me create a BN
57:09
mean underscore running and I'm going to initialize it at zeros
57:15
and then be an STD running which I'll initialize at once
57:23
because in the beginning because of the way we initialized W1 and B1 each react will be
57:30
roughly unit gaussian so the mean will be roughly zero and the standard deviation roughly one so I'm going to
57:35
initialize these that way but then here I'm going to update these and in pytorch
57:41
these mean and standard deviation that are running they're not actually part of the gradient based optimization we're
57:47
never going to derive gradients with respect to them they're they're updated on the side of training
57:53
and so what we're going to do here is we're going to say with torch top no grad telling pytorch that the update
58:00
here is not supposed to be building out a graph because there will be no doubt backward but this running is basically going to
58:07
be 0.99 times the current value
58:13
plus 0.001 times the this value this new mean
58:20
and in the same way be an STD running will be mostly what it used to be
58:28
but it will receive a small update in the direction of what the current standard deviation is
58:34
and as you're seeing here this update is outside and on the side of the gradient based optimization and it's simply being
58:42
updated not using gradient descent it's just being updated using a gen key like Smooth
58:48
sort of running mean manner and so while the network is training and
58:55
these pre-activations are sort of changing and shifting around during back propagation we are keeping track of the
59:02
typical mean and standard deviation and we're estimating them once and when I run this
59:09
now I'm keeping track of this in a running manner and what we're hoping for of course is that the meat being mean
59:14
underscore running and B and mean underscore STD are going to be very similar to the ones that we calculated
59:20
here before and that way we don't need a second stage because we've sort of combined the
59:26
two stages and we've put them on the side of each other if you want to look at it that way and this is how this is also implemented
59:32
in the batch normalization layer in pi torch so during training um the exact same thing will happen and
59:39
then later when you're using inference it will use the estimated running mean of both the mean estimate deviation of
59:46
those hidden States so let's wait for the optimization to converge and hopefully the running mean
59:51
and standard deviation are roughly equal to these two and then we can simply use it here and we don't need this stage of
59:57
explicit calibration at the end okay so the optimization finished I'll rerun the explicit estimation and
1:00:04
then the B and mean from the explicit estimation is here and B and mean from the running
1:00:09
estimation during the during the optimization you can see it's very very similar
1:00:16
it's not identical but it's pretty close in the same way be an STD is this and be
1:00:23
an STD running is this as you can see that once again they are
1:00:28
fairly similar values not identical but pretty close and so then here instead of being mean
1:00:33
we can use the B and mean running instead of being STD we can use bnstd running
1:00:39
and uh hopefully the validation loss will not be impacted too much okay so it's basically identical and
1:00:47
this way we've eliminated the need for this explicit stage of calibration because we are doing it in line over
1:00:53
here okay so we're almost done with batch normalization there are only two more notes that I'd like to make number
1:00:58
one I've skipped a discussion over what is this plus Epsilon doing here this Epsilon is usually like some small fixed
1:01:04
number for example one a negative five by default and what it's doing is that it's basically preventing a division by
1:01:09
zero in the case that the variance over your batch is exactly zero in that case uh here we
1:01:17
normally have a division by zero but because of the plus Epsilon this is going to become a small number in the denominator instead and things will be
1:01:24
more well behaved so feel free to also add a plus Epsilon here of a very small number it doesn't actually substantially
1:01:30
change the result I'm going to skip it in our case just because this is unlikely to happen in our very simple example here and the second thing I want
1:01:37
you to notice is that we're being wasteful here and it's very subtle but right here where we are adding the bias
1:01:43
into hpact these biases now are actually useless because we're adding them to the hpact
1:01:49
but then we are calculating the mean for every one of these neurons and subtracting it so whatever bias you add
1:01:57
here is going to get subtracted right here and so these biases are not doing anything in fact they're being
1:02:03
subtracted out and they don't impact the rest of the calculation so if you look at b1.grad it's actually going to be
1:02:09
zero because it's being subtracted out and doesn't actually have any effect and so whenever you're using batch
1:02:14
normalization layers then if you have any weight layers before like a linear or a comma or something like that you're
1:02:21
better off coming here and just like not using bias so you don't want to use bias and then here you don't want to add it
1:02:28
because it's that spurious instead we have this vast normalization bias here and that bastionalization bias is now in
1:02:36
charge of the biasing of this distribution instead of this B1 that we had here originally
1:02:42
and so basically the rationalization layer has its own bias and there's no need to have a bias in the layer before
1:02:48
it because that bias is going to be extracted up anyway so that's the other small detail to be careful with sometimes it's not going to
1:02:55
do anything catastrophic this B1 will just be useless it will never get any gradient it will not learn it will stay
1:03:01
constant and it's just wasteful but it doesn't actually really impact anything otherwise okay so I rearranged the code
batch normalization: summary
1:03:08
a little bit with comments and I just wanted to give a very quick summary of the bachelorization layer
1:03:13
we are using batch normalization to control the statistics of activations in the neural net
1:03:19
it is common to sprinkle batch normalization layer across the neural net and usually we will place it after
1:03:25
layers that have multiplications like for example a linear layer or a convolutional layer which we may cover
1:03:31
in the future now the batch normalization internally has parameters for the gain and the bias
1:03:39
and these are trained using back propagation it also has two buffers the buffers are
1:03:45
the mean and the standard deviation the running mean and the running mean of the standard deviation
1:03:50
and these are not trained using back propagation these are trained using this janky update of kind of like a running
1:03:57
mean update so um these are sort of the parameters and the
1:04:03
buffers of bashram layer and then really what it's doing is it's calculating the mean and the standard deviation of the
1:04:09
activations uh that are feeding into the bathroom layer over that batch
1:04:14
then it's centering that batch to be unit gaussian and then it's offsetting and scaling it by the Learned bias and
1:04:22
Gain and then on top of that it's keeping track of the mean and standard deviation of the inputs
1:04:28
and it's maintaining this running mean and standard deviation and this will later be used at inference
1:04:34
so that we don't have to re-estimate the meanest standard deviation all the time and in addition that allows us to
1:04:40
basically forward individual examples at test time so that's the batch normalization layer it's a fairly complicated layer
1:04:48
um but this is what it's doing internally now I wanted to show you a little bit of a real example
real example: resnet50 walkthrough
1:04:53
so you can search resnet which is a residual neural network and these are
1:04:58
context of neural networks used for image classification and of course we haven't come dresnets
1:05:04
in detail so I'm not going to explain all the pieces of it but for now just note that the image feeds into a resnet
1:05:11
on the top here and there's many many layers with repeating structure all the way to predictions of what's inside that
1:05:17
image this repeating structure is made up of these blocks and these blocks are just sequentially stacked up in this deep
1:05:24
neural network now the code for this the block basically that's used and repeated
1:05:30
sequentially in series is called this bottleneck block bottleneck block
1:05:36
and there's a lot here this is all pytorch and of course we haven't covered all of it but I want to point out some
1:05:41
small pieces of it here in the init is where we initialize the neural net so this coded block here
1:05:47
is basically the kind of stuff we're doing here we're initializing all the layers and in the forward we are specifying how
1:05:53
the neural lot acts once you actually have the input so this code here is along the lines of what we're doing here
1:06:01
and now these blocks are replicated and stacked up serially and that's what a
1:06:06
residual Network would be and so notice what's happening here com1
1:06:12
these are convolutional layers and these convolutional layers basically they're the same thing as a linear layer
1:06:19
except convolutional layers don't apply um convolutional layers are used for images and so they have spatial
1:06:25
structure and basically this linear multiplication and bias offset are done on patches
1:06:31
instead of a math instead of the full input so because these images have structure spatial structure convolutions
1:06:38
just basically do WX plus b but they do it on overlapping patches of the input
1:06:43
but otherwise it's WX plus b then we have the norm layer which by default here is initialized to be a
1:06:50
batch Norm in 2D so two-dimensional batch normalization layer and then we have a nonlinearity like
1:06:56
relu so instead of uh here they use relu we are using 10h in this case
1:07:02
but both both are just nonlinearities and you can just use them relatively interchangeably from very deep networks
1:07:08
relu is typically empirically work a bit better so see the motif that's being repeated
1:07:13
here we have convolution batch normalization convolution patch normalization early Etc and then here
1:07:19
this is residual connection that we haven't covered yet but basically that's the exact same pattern we have here we have a weight
1:07:27
layer like a convolution or like a linear layer batch normalization and
1:07:32
then 10h which is a nonlinearity but basically a weight layer a normalization
1:07:37
layer and a nonlinearity and that's the motif that you would be stacking up when you create these deep neural networks
1:07:43
exactly as it's done here and one more thing I'd like you to notice is that here when they are initializing the comp
1:07:49
layers like comp one by one the depth for that is right here and so it's initializing an nn.cap2d
1:07:56
which is a convolutional layer in pytorch and there's a bunch of keyword arguments here that I'm not going to explain yet but you see how there's bias
1:08:03
equals false the bicycles fall is exactly for the same reason as bias is not used in our case the CRI race to use
1:08:11
a bias and these are bias is spurious because after this weight layer there's a bachelorization and the bachelor
1:08:17
normalization subtracts that bias and then has its own bias so there's no need to introduce these spurious parameters
1:08:22
it wouldn't hurt performance it's just useless and so because they have this motif of
1:08:28
calf pasture and relu they don't need to buy us here because there's a bias inside here
1:08:33
so by the way this example here is very easy to find just do resnet pie torch
1:08:39
and uh it's this example here so this is kind of like the stock implementation of a residual neural network in pytorch and
1:08:46
you can find that here but of course I haven't covered many of these parts yet and I would also like to briefly descend
1:08:52
into the definitions of these pytorch layers and the parameters that they take now instead of a convolutional layer
1:08:58
we're going to look at a linear layer uh because that's the one that we're using here this is a linear layer and I
1:09:04
haven't covered convolutions yet but as I mentioned convolutions are basically linear layers except on patches
1:09:11
so a linear layer performs a w x plus b except here they're calling the W A transpose
1:09:18
um since is WX plus b very much like we did here to initialize this layer you need to know the fan in the
1:09:24
fan out and that's so that they can initialize this W this is the fan in and the fan
1:09:31
out so they know how how big the weight Matrix should be you need to also pass in whether you
1:09:36
whether or not you want a bias and if you set it to false then no bias will be inside this layer
1:09:44
um and you may want to do that exactly like in our case if your layer is followed by a normalization layer such
1:09:49
as Bachelor so this allows you to basically disable bias now in terms of the initialization if we
1:09:56
swing down here this is reporting the variables used inside this linear layer and our linear layer here has two
1:10:03
parameters the weight and the bias in the same way they have a weight and a bias and they're talking about how they
1:10:09
initialize it by default so by default python initialize your weights by taking the fan in
1:10:16
and then doing one over Fannin square root and then instead of a normal
1:10:22
distribution they are using a uniform distribution so it's very much the same thing but
1:10:28
they are using a one instead of five over three so there's no gain being calculated here the gain is just one but
1:10:34
otherwise it's exactly one over the square root of fan in exactly as we have here
1:10:40
so 1 over the square root of K is the is the scale of the weights but when they
1:10:46
are drawing the numbers they're not using a gaussian by default they're using a uniform distribution by default
1:10:51
and so they draw uniformly from negative square root of K to square root of K but it's the exact same thing and the
1:10:58
same motivation from for with respect to what we've seen in this lecture and the
1:11:03
reason they're doing this is if you have a roughly gaussian input this will ensure that out of this layer you will
1:11:09
have a roughly gaussian output and you you basically achieve that by scaling
1:11:14
the weights by 100 square root of fan in so that's what this is doing and then the second thing is the battery
1:11:22
normalization layer so let's look at what that looks like in pytorch so here we have a one-dimensional mesh
1:11:27
normalization layer exactly as we are using here and there are a number of keyword arguments going into it as well
1:11:33
so we need to know the number of features uh for us that is 200 and that is needed so that we can initialize
1:11:39
these parameters here the gain the bias and the buffers for the running mean and
1:11:45
standard deviation then they need to know the value of Epsilon here and by default this is one
1:11:51
negative five you don't typically change this too much then they need to know the momentum and the momentum here as they
1:11:57
explain is basically used for these uh running mean and running standard deviation
1:12:02
so by default the momentum here is 0.1 the momentum we are using here in this example is 0.001
1:12:09
and basically rough you may want to change this sometimes and roughly speaking if you have a very large batch
1:12:16
size then typically what you'll see is that when you estimate the mean and the standard deviation
1:12:21
for every single batch size if it's large enough you're going to get roughly the same result and so therefore you can use slightly
1:12:28
higher momentum like 0.1 but for a batch size as small as 32 the
1:12:34
mean understand deviation here might take on slightly different numbers because there's only 32 examples we are using to estimate the mean of standard
1:12:41
deviation so the value is changing around a lot and if your momentum is 0.1 that that might not be good enough for
1:12:48
this value to settle and converge to the actual mean and standard deviation over the entire
1:12:53
training set and so basically if your batch size is very small momentum of 0.1 is potentially dangerous and it might make
1:13:00
it so that the running mean and standard deviation is thrashing too much during training and it's not actually
1:13:06
converging properly uh Alpha and equals true determines
1:13:11
whether dispatch normalization layer has these learnable affine parameters the uh
1:13:16
the gain and the bias and this is almost always kept it true I'm not actually sure why you would want to change this
1:13:23
to false um then track running stats is determining
1:13:28
whether or not bachelorization layer of pytorch will be doing this and one reason you may you may want to
1:13:35
skip the running stats is because you may want to for example estimate them at the end as a stage two like this and in
1:13:43
that case you don't want the batch normalization layer to be doing all this extra compute that you're not going to use
1:13:48
and finally we need to know which device we're going to run this batch normalization on a CPU or a GPU and what
1:13:55
the data type should be uh half Precision single Precision double precision and so on
1:14:00
so that's the batch normalization layer otherwise the link to the paper is the same formula we've implement it and
1:14:06
everything is the same exactly as we've done here okay so that's everything that I wanted
summary of the lecture
1:14:12
to cover for this lecture really what I wanted to talk about is the importance of understanding the activations and the
1:14:17
gradients and their statistics in neural networks and this becomes increasingly important especially as you make your
1:14:23
neural networks bigger larger and deeper we looked at the distributions basically at the output layer and we saw that if
1:14:29
you have two confident mispredictions because the activations are too messed up at the last layer you can end up with
1:14:35
these hockey stick losses and if you fix this you get a better loss at the end of training because your training is not
1:14:42
doing wasteful work then we also saw that we need to control the activations we don't want them to you know squash to zero or explode to
1:14:49
infinity and because that you can run into a lot of trouble with all of these non-linearities in these neural nuts and
1:14:56
basically you want everything to be fairly homogeneous throughout the neural net you want roughly gaussian activations throughout the neural net
1:15:02
let me talk about okay if we would roughly gaussian activations how do we
1:15:07
scale these weight matrices and biases during initialization of the neural net so that we don't get um you know so
1:15:13
everything is as controlled as possible um so that gave us a large boost in
1:15:19
Improvement and then I talked about how that strategy is not actually uh
1:15:24
possible for much much deeper neural Nets because when you have much deeper
1:15:29
neural nuts with lots of different types of layers it becomes really really hard to precisely set the weights and the
1:15:36
biases in such a way that the activations are roughly uniform throughout the neural net
1:15:41
so then I introduced the notion of the normalization layer now there are many normalization layers that people use in
1:15:47
practice batch normalization layer normalization constant normalization group normalization we haven't covered
1:15:53
most of them but I've introduced the first one and also the one that I believe came out first and that's called
1:15:58
batch normalization and we saw how batch normalization works this is a layer that you can sprinkle
1:16:04
throughout your deep neural nut and the basic idea is if you want roughly gaussian activations well then take your
1:16:11
activations and take the mean understand deviation and Center your data and you
1:16:17
can do that because the centering operation is differentiable but and on top of that we actually had
1:16:23
to add a lot of bells and whistles and that gave you a sense of the complexities of the patch normalization layer because now we're centering the
1:16:30
data that's great but suddenly we need the gain and the bias and now those are trainable
1:16:35
and then because we are coupling all the training examples now suddenly the question is how do you do the inference or to do to do the inference we need to
1:16:43
now estimate these mean and standard deviation once or the entire training
1:16:49
set and then use those at inference but then no one likes to do stage two so instead we fold everything into the
1:16:56
batch normalization layer during training and try to estimate these in the running manner so that everything is
1:17:01
a bit simpler and that gives us the batch normalization layer um and as I mentioned no one likes this
1:17:08
layer it causes a huge amount of bugs um and intuitively it's because it is
1:17:14
coupling examples in the forward pass of a neural net and I've shot myself in the
1:17:21
foot with this layer over and over again in my life and I don't want you to suffer the same
1:17:28
uh so basically try to avoid it as much as possible uh some of the other alternatives to these layers are for
1:17:34
example group normalization or layer normalization and those have become more common uh in more recent deep learning
1:17:40
but we haven't covered those yet but definitely batch normalization was very influential at the time when it came out
1:17:46
in roughly 2015 because it was kind of the first time that you could train reliably uh much deeper neural nuts and
1:17:55
fundamentally the reason for that is because this layer was very effective at controlling the statistics of the
1:18:01
activations in a neural net so that's the story so far and um that's
1:18:06
all I wanted to cover and in the future lectures hopefully we can start going into recurring neural Nets and recurring
1:18:13
neural Nets as we'll see are just very very deep networks because you uh you unroll the loop and uh when you actually
1:18:19
optimize these neurons and that's where a lot of this um analysis around the activation
1:18:26
statistics and all these normalization layers will become very very important for a good performance so we'll see that
1:18:33
next time okay so I lied I would like us to do one more summary here as a bonus and I think
just kidding: part2: PyTorch-ifying the code
1:18:40
it's useful as to have one more summary of everything I've presented in this lecture but also I would like us to
1:18:45
start by tortifying our code a little bit so it looks much more like what you would encounter in pi torch so you'll
1:18:50
see that I will structure our code into these modules like a linear module and a
1:18:57
bachelor module and I'm putting the code inside these modules so that we can construct neural networks very much like
1:19:03
we would construct them in pytorch and I will go through this in detail so we'll create our neural net
1:19:08
then we will do the optimization loop as we did before and then the one more thing that I want
1:19:13
to do here is I want to look at the activation statistics both in the forward pass and in the backward pass and then here we have the evaluation and
1:19:21
sampling just like before so let me rewind all the way up here and go a little bit slower
1:19:26
so here I'm creating a linear layer you'll notice that torch.nn has lots of different types of layers and one of
1:19:33
those layers is the linear layer linear takes a number of input features output features whether or not we should
1:19:39
have bias and then the device that we want to place this layer on and the data type so I will omit these two but
1:19:46
otherwise we have the exact same thing we have the fan in which is number of inputs fan out the number of outputs and
1:19:53
whether or not we want to use a bias and internally inside this layer there's a weight and a bias if you like it
1:19:59
it is typical to initialize the weight using say random numbers drawn from a
1:20:05
gaussian and then here's the coming initialization that we've discussed already in this lecture and that's a
1:20:11
good default and also the default that I believe python trees is and by default the bias is usually initialized to zeros
1:20:18
now when you call this module this will basically calculate W Times X plus b if
1:20:23
you have NB and then when you also call that parameters on this module it will return the tensors that are the parameters of
1:20:31
this layer now next we have the bachelorization layer so I've written that here and this
1:20:38
is very similar to Pi torch NN dot bash Norm 1D layer as shown here
1:20:44
so I'm kind of taking these three parameters here the dimensionality the
1:20:49
Epsilon that we'll use in the division and the momentum that we will use in keeping track of these running stats the
1:20:55
running mean and the running variance um now pack torch actually takes quite a few more things but I'm assuming some of
1:21:01
their settings so for us I find will be true that means that we will be using a gamma and beta after the normalization
1:21:07
the track running stats will be true so we will be keeping track of the running mean and the running variance in the in
1:21:13
the pattern our device by default is the CPU and the data type by default is a float
1:21:20
float32. so those are the defaults otherwise we are taking all the same parameters in
1:21:26
this bathroom layer so first I'm just saving them now here's something new there's a DOT
1:21:31
training which by default is true in packtorch and then modules also have this attribute that training and that's
1:21:37
because many modules and batch Norm is included in that have a different behavior of whether you are training
1:21:44
your own lot and or whether you are running it in an evaluation mode and calculating your evaluation loss or
1:21:49
using it for inference on some test examples and masterm is an example of this
1:21:54
because when we are training we are going to be using the mean and the variance estimated from the current batch but during inference we are using
1:22:01
the running mean and running variants and so also if we are training we are updating mean and variants but if we are
1:22:08
testing then these are not being updated they're kept fixed and so this flag is necessary and by
1:22:13
default true just like impact torch now the parameters investment 1D are the gamma and the beta here
1:22:21
and then the running mean and running variants are called buffers in pytorch nomenclature and these buffers are
1:22:29
trained using exponential moving average here explicitly and they are not part of
1:22:34
the back propagation and stochastic gradient descent so they are not sort of like parameters of this layer and that's
1:22:40
why when we calculate when we have a parameters here we only return gamma and beta we do not return the mean and the
1:22:46
variance this is trained sort of like internally here every forward pass using
1:22:51
exponential moving average so that's the initialization now in a forward pass if we are training
1:22:59
then we use the mean and the variance estimated by the batch let me plot the paper here
1:23:05
we calculate the mean and the variance now up above I was estimating the
1:23:11
standard deviation and keeping track of the standard deviation here in the running standard deviation instead of
1:23:16
running variance but let's follow the paper exactly here they calculate the variance which is the standard deviation
1:23:23
squared and that's what's kept track of in the running variance instead of a running standard deviation
1:23:29
uh but those two would be very very similar I believe if we are not training then we use
1:23:35
running mean in various we normalize and then here I'm calculating the output
1:23:41
of this layer and I'm also assigning it to an attribute called dot out now dot out is something that I'm using
1:23:47
in our modules here this is not what you would find in pytorch we are slightly deviating from it I'm creating a DOT out
1:23:54
because I would like to very easily maintain all those variables so that we can create statistics of them and plot
1:24:00
them but Pi torch and modules will not have a data attribute then finally here we are updating the
1:24:06
buffers using again as I mentioned exponential moving average uh provide given the provided momentum
1:24:12
and importantly you'll notice that I'm using the torstart no grad context manager and I'm doing this because if we
1:24:18
don't use this then pytorch will start building out an entire computational graph out of these tensors because it is
1:24:25
expecting that we will eventually call it that backward but we are never going to be calling that backward on anything
1:24:30
that includes running mean and running variance so that's why we need to use this contact manager so that we are not
1:24:36
sort of maintaining them using all this additional memory so this will make it
1:24:41
more efficient and it's just telling factors that will only know backward we just have a bunch of tensors we want to update them that's it
1:24:47
and then we return okay now scrolling down we have the 10h layer this is very very similar to
1:24:54
torch.10h and it doesn't do too much it just calculates 10h as you might expect
1:25:00
so that's torch.nh and there's no parameters in this layer
1:25:05
but because these are layers um it now becomes very easy to sort of like stack them up into basically just a
1:25:11
list and we can do all the initializations that we're used to so we have the
1:25:17
initial sort of embedding Matrix we have our layers and we can call them sequentially and then again with Trump shot no grad
1:25:24
there's some initializations here so we want to make the output softmax a bit less confident like we saw and in
1:25:30
addition to that because we are using a six layer multi-layer perceptron here so you see how I'm stacking linear 10 age
1:25:37
linear 10h Etc I'm going to be using the game here and I'm going to play with this in a second
1:25:42
so you'll see how when we change this what happens to the statistics finally the primers are basically the
1:25:49
embedding Matrix and all the parameters in all the layers and notice here I'm using a double list comprehension if you
1:25:55
want to call it that but for every layer in layers and for every parameter in each of those layers we are just
1:26:01
stacking up all those piece all those parameters now in total we have 46 000 parameters
1:26:09
and I'm telling by George that all of them require gradient
1:26:15
then here we have everything here we are actually mostly used to we are sampling
1:26:21
batch we are doing forward pass the forward pass now is just a linear application of all the layers in order
1:26:27
followed by the cross entropy and then in the backward path you'll notice that for every single layer I now
1:26:32
iterate over all the outputs and I'm telling pytorch to retain the gradient of them and then here we are already used to all
1:26:40
the all the gradients set To None do the backward to fill in the gradients do an update using the caskaranian scent and
1:26:46
then track some statistics and then I am going to break after a single iteration now here in this cell in this diagram
viz #1: forward pass activations statistics
1:26:53
I'm visualizing the histogram the histograms of the forward pass activations and I'm specifically doing
1:26:59
it at the 10 inch layers so iterating over all the layers except for the very last one which is basically
1:27:06
just the soft Max layer um if it is a 10 inch layer and I'm
1:27:12
using a 10 inch layer just because they have a finite output negative one to one and so it's very easy to visualize here
1:27:17
so you see negative one to one and it's a finite range and easy to work with I take the out tensor from that layer
1:27:24
into T and then I'm calculating the mean the standard deviation and the percent saturation of t
1:27:30
and the way I Define the percent saturation is that t dot absolute value is greater than 0.97 so that means we
1:27:36
are here at the Tails of the 10h and remember that when we are in the Tails of the 10h that will actually stop
1:27:41
gradients so we don't want this to be too high now here I'm calling torch.histogram and
1:27:49
then I am plotting this histogram so basically what this is doing is that every different type of layer and they all have a different color we are
1:27:55
looking at how many um values in these tensors take on any of the values Below on this axis here
1:28:04
so the first layer is fairly saturated here at 20 so you can see that it's got
1:28:09
Tails here but then everything sort of stabilizes and if we had more layers here it would actually just stabilize at
1:28:15
around the standard deviation of about 0.65 and the saturation would be roughly five percent
1:28:20
and the reason that this stabilizes and gives us a nice distribution here is because gain is set to 5 over 3.
1:28:27
now here this gain you see that by default we initialize with one over
1:28:33
square root of fan in but then here during initialization I come in and I iterate all the layers and if it's a
1:28:39
linear layer I boost that by the gain now we saw that one so basically if we
1:28:45
just do not use a gain then what happens if I redraw this you will see that
1:28:52
the standard deviation is shrinking and the saturation is coming to zero and
1:28:57
basically what's happening is the first layer is you know pretty decent but then further layers are just kind of like
1:29:03
shrinking down to zero and it's happening slowly but it's shrinking to zero and the reason for that is when you
1:29:10
just have a sandwich of linear layers alone then a then initializing our
1:29:16
weights in this manner we saw previously would have conserved the standard deviation of one
1:29:22
but because we have this interspersed 10h layers in there the Stanley layers are squashing
1:29:28
functions and so they take your distribution and they slightly squash it and so some gain is necessary to keep
1:29:36
expanding it to fight the squashing so it just turns out that 5 over 3 is a
1:29:42
good value so if we have something too small like one we saw that things will come towards zero but if it's something
1:29:49
too high let's do two then here we see that um
1:29:56
well let me do something a bit more extreme because so it's a bit more visible let's try three
1:30:02
okay so we see here that the saturations are trying to be way too large okay so three would create way too
1:30:08
saturated activations so five over three is a good setting for
1:30:14
a sandwich of linear layers with 10 inch activations and it roughly stabilizes
1:30:19
the standard deviation at a reasonable point now honestly I have no idea where five
1:30:24
over three came from in pytorch when we were looking at the coming initialization I see empirically that it
1:30:31
stabilizes this sandwich of linear n10h and that the saturation is in a good range but I don't actually know this
1:30:38
came out of some math formula I tried searching briefly for where this comes from but I wasn't able to find anything
1:30:44
but certainly we see that empirically these are very nice ranges our saturation is roughly five percent which
1:30:49
is a pretty good number and uh this is a good setting of The gain in this context similarly we can do the exact same thing
viz #2: backward pass gradient statistics
1:30:57
with the gradients so here is a very same Loop if it's a 10h but instead of taking the layered that out I'm taking
1:31:03
the grad and then I'm also showing the mean on the standard deviation and I'm plotting the histogram of these values
1:31:09
and so you'll see that the gradient distribution is fairly reasonable and in particular what we're looking for is
1:31:15
that all the different layers in this sandwich has roughly the same gradient things are not shrinking or exploding
1:31:21
so we can for example come here and we can take a look at what happens if this gain was way too small so this was 0.5
1:31:30
then you see the first of all the activations are shrinking to zero but also the gradients are doing something weird the gradient
1:31:36
started out here and then now they're like expanding out and similarly if we for example have a
1:31:43
too high again so like three then we see that also the gradients have there's some asymmetry going on where as
1:31:50
you go into deeper and deeper layers the activations are also changing and so that's not what we want and in this case
1:31:56
we saw that without the use of Bachelor as we are going through right now we have to very carefully set those gains
1:32:02
to get nice activations in both the forward pass and the backward pass now before we move on to pasture
the fully linear case of no non-linearities
1:32:09
normalization I would also like to take a look at what happens when we have no 10h units here so erasing all the 10
1:32:15
inch nonlinearities but keeping the gain at 5 over 3. we now have just a giant linear sandwich so
1:32:22
let's see what happens to the activations as we saw before the correct gain here is one that is the standard deviation
1:32:28
preserving gain so 1.667 is too high and so what's going to
1:32:34
happen now is the following I have to change this to be linear so we
1:32:39
are because there's no more 10 inch layers and let me change this to linear as well
1:32:46
so what we're seeing is um the activations started out on the blue and have by layer 4 become very
1:32:54
diffuse so what's happening to the activations is this and with the gradients on the top layer
1:33:00
the activation the gradient statistics are the purple and then they diminish as
1:33:05
you go down deeper in the layers and so basically you have an asymmetry like in the neural net and you might imagine
1:33:11
that if you have very deep neural networks say like 50 layers or something like that this just this is not a good
1:33:17
place to be so that's why before bash normalization this was incredibly tricky
1:33:22
to to set in particular if this is too large of a game this happens and if it's too little it can gain
1:33:29
then this happens also the opposite of that basically happens here we have a um
1:33:36
shrinking and a diffusion depending on which direction you look at it from
1:33:42
and so certainly this is not what you want and in this case the correct setting of The gain is exactly one
1:33:48
just like we're doing at initialization and then we see that the statistics for the forward and the
1:33:54
backward pass are well behaved and so the reason I want to show you this is the basically like getting neuralness to
1:34:02
train before these normalization layers and before the use of advanced optimizers like atom which we still have
1:34:07
to cover and residual connections and so on training neurons basically look like this it's like a total Balancing Act you
1:34:15
have to make sure that everything is precisely orchestrated and you have to care about the activations and the gradients and their statistics and then
1:34:21
maybe you can train something but it was basically impossible to train very deep networks and this is fundamentally the
1:34:26
reason for that you'd have to be very very careful with your initialization
1:34:32
um the other point here is you might be asking yourself by the way I'm not sure if I covered this why do we need these
1:34:38
10h layers at all why do we include them and then have to worry about the gain
1:34:43
and uh the reason for that of course is that if you just have a stack of linear layers then certainly we're getting very easily
1:34:49
nice activations and so on but this is just a massive linear sandwich and it
1:34:54
turns out that it collapses to a single linear layer in terms of its representation power so if you were to
1:35:00
plot the output as a function of the input you're just getting a linear function no matter how many linear layers you stack up you still just end
1:35:07
up with a linear transformation all the W X Plus B's just collapse into a large
1:35:12
WX plus b with slightly different W's as likely different B um but interestingly even though the
1:35:18
forward pass collapses to just a linear layer because of back propagation and the Dynamics of the backward pass the
1:35:26
optimization is really is not identical you actually end up with all kinds of interesting
1:35:31
um Dynamics in the backward pass because of the uh the way the chain rule is
1:35:36
calculating it and so optimizing a linear layer by itself and optimizing a
1:35:42
sandwich of 10 millionaire layers in both cases those are just a linear transformation in the forward pass but
1:35:47
the training Dynamics would be different and there's entire papers that analyze in fact like infinitely layered linear
1:35:53
layers and so on and so there's a lot of things too that you can play with there uh but basically the technical
1:35:59
linearities allow us to um turn this sandwich from just a linear
1:36:07
function into a neural network that can in principle approximate any arbitrary
1:36:14
function okay so now I've reset the code to use the linear 10h sandwich like before and
viz #3: parameter activation and gradient statistics
1:36:20
I reset everything so the gains five over three we can run a single step of optimization and we can look at the
1:36:27
activation statistics of the forward pass and the backward pass but I've added one more plot here that I
1:36:32
think is really important to look at when you're training your neural nuts and to consider and ultimately what we're doing is we're updating the
1:36:38
parameters of the neural net so we care about the parameters and their values and their gradients
1:36:44
so here what I'm doing is I'm actually iterating over all the parameters available and then I'm only
1:36:49
um restricting it to the two-dimensional parameters which are basically the weights of these linear layers and I'm
1:36:55
skipping the biases and I'm skipping the um Gammas and the betas in the bathroom
1:37:00
just for Simplicity but you can also take a look at those as well but what's happening with the
1:37:05
weights is um instructive by itself so here we have all the different weights their shapes so this is the
1:37:13
embedding layer the first linear layer all the way to the very last linear layer and then we have the mean the
1:37:18
standard deviation of all these parameters the histogram and you can see that it actually doesn't look that amazing so
1:37:25
there's some trouble in Paradise even though these gradients looked okay there's something weird going on here I'll get to that in a second and the
1:37:32
last thing here is the gradient to data ratio so sometimes I like to visualize this as well because what this gives you
1:37:39
a sense of is what is the scale of the gradient compared to the scale of the actual values and this is important
1:37:46
because we're going to end up taking a step update that is the learning rate times the
1:37:52
gradient onto the data and so the gradient has two large of magnitude if the numbers in there are too large
1:37:58
compared to the numbers in data then you'd be in trouble but in this case the gradient to data is
1:38:04
our loan numbers so the values inside grad are 1000 times smaller than the
1:38:09
values inside data in these weights most of them now notably that is not true about the
1:38:16
last layer and so the last layer actually here the output layer is a bit of a troublemaker in the way that this
1:38:21
is currently arranged because you can see that the um the last layer here in pink takes on
1:38:29
values that are much larger than some of the values inside
1:38:34
um inside the neural net so the standard deviations are roughly 1 and negative three throughout except for the last
1:38:40
last layer which actually has roughly one e negative two a standard deviation of gradients and so the gradients on the
1:38:47
last layer are currently about 100 times greater sorry 10 times greater than all
1:38:52
the other weights inside the neural nut and so that's problematic because in the
1:38:58
simple stochastically in the sun setup you would be training this last layer about 10 times faster than you would be
1:39:04
training the other layers at initialization now this actually like kind of fixes itself a little bit if you train for a
1:39:10
bit longer so for example if I greater than 1000 only then do a break
1:39:16
let me reinitialize and then let me do it 1000 steps and after 1000 steps we
1:39:21
can look at the forward pass okay so you see how the neurons are a bit are saturating a bit and we can also
1:39:28
look at the backward pass but otherwise they look good they're about equal and there's no shrinking to zero or
1:39:34
exploding to infinities and you can see that here in the weights things are also stabilizing a little bit
1:39:40
so the Tails of the last pink layer are actually coming down coming in during the optimization
1:39:46
but certainly this is like a little bit of troubling especially if you are using a very simple update rule like
1:39:51
stochastic gradient descent instead of a modern Optimizer like Adam now I'd like to show you one more plot
viz #4: update:data ratio over time
1:39:56
that I usually look at when I train neural networks and basically the gradient to data ratio is not actually
1:40:02
that informative because what matters at the end is not the gradient to date ratio but the update to the data ratio
1:40:08
because that is the amount by which we will actually change the data in these tensors so coming up here what I'd like to do is
1:40:15
I'd like to introduce a new update to data ratio it's going to be less than we're going
1:40:21
to build it out every single iteration and here I'd like to keep track of basically the ratio
1:40:27
every single iteration so without any gradients I'm comparing the
1:40:34
update which is learning rate times the times the gradient that is the update that we're going to
1:40:40
apply to every parameter social Mediterranean or all the parameters and then I'm taking the
1:40:45
basically standard deviation of the update we're going to apply and divide it by the actual content the data of
1:40:53
that parameter and its standard deviation so this is the ratio of basically how
1:40:58
great are the updates to the values in these tensors then we're going to take a log of it and
1:41:03
actually I'd like to take a log 10. um just so it's a nicer visualization so
1:41:10
we're going to be basically looking at the exponents of the of this division here and then that item
1:41:17
to pop out the float and we're going to be keeping track of this for all the parameters and adding it to this UD
1:41:23
tensor so now let me reinitialize and run a thousand iterations we can look at the activations the
1:41:31
gradients and the parameter gradients as we did before but now I have one more plot here to introduce
1:41:37
now what's happening here is where every interval go to parameters and I'm constraining it again like I did here to
1:41:42
just the weights so the number of dimensions in these sensors is two and then I'm basically
1:41:48
plotting all of these update ratios over time
1:41:54
so when I plot this I plot those ratios and you can see that they evolve over time during
1:41:59
initialization to take on certain values and then these updates sort of like start stabilizing usually during
1:42:04
training then the other thing that I'm plotting here is I'm plotting here like an approximate value that is a Rough Guide
1:42:10
for what it roughly should be and it should be like roughly one in negative three and so that means that basically there's
1:42:17
some values in this tensor um and they take on certain values and the updates to them at every single
1:42:23
iteration are no more than roughly 1 000 of the actual like magnitude in those
1:42:29
tensors uh if this was much larger like for example if this was
1:42:34
um if the log of this was like say negative one this is actually updating those values quite a lot they're undergoing a
1:42:40
lot of change but the reason that the final rate the final layer here is an outlier is
1:42:46
because this layer was artificially shrunk down to keep the soft max income
1:42:52
unconfident so here you see how we multiply The Weight by
1:42:57
point one uh in the initialization to make the last layer prediction less confident
1:43:03
that made that artificially made the values inside that tensor way too low and that's why we're getting temporarily
1:43:10
a very high ratio but you see that that stabilizes over time once that weight starts to learn starts to learn
1:43:17
but basically I like to look at the evolution of this update ratio for all my parameters usually and I like to make
1:43:23
sure that it's not too much above wanting negative three roughly
1:43:29
uh so around negative three on this log plot if it's below negative three usually
1:43:34
that means that the parameters are not training fast enough so if our learning rate was very low let's do that experiment
1:43:41
let's initialize and then let's actually do a learning rate of say y negative three here
1:43:47
so 0.001 if you're learning rate is way too low
1:43:53
this plot will typically reveal it so you see how all of these updates are
1:43:59
way too small so the size of the update is basically uh 10 000 times
1:44:05
in magnitude to the size of the numbers in that tensor in the first place so
1:44:10
this is a symptom of training way too slow so this is another way to sometimes set
1:44:16
the learning rate and to get a sense of what that learning rate should be and ultimately this is something that you would keep track of
1:44:24
if anything the learning rate here is a little bit on the higher side because
1:44:29
you see that um we're above the black line of negative three we're somewhere around negative 2.5 it's like okay and uh but
1:44:38
everything is like somewhat stabilizing and so this looks like a pretty decent setting of of um learning rates and so on but this is
1:44:44
something to look at and when things are miscalibrated you will you will see very quickly so for example
1:44:50
everything looks pretty well behaved right but just as a comparison when things are not properly calibrated what does that look like let me come up here
1:44:57
and let's say that for example uh what do we do let's say that we forgot to apply this
1:45:04
fan in normalization so the weights inside the linear layers are just a sample for my gaussian in all those
1:45:09
stages what happens to our how do we notice that something's off well the activation plot will tell you
1:45:16
whoa your neurons are way too saturated the gradients are going to be all messed up the histogram for these weights are
1:45:22
going to be all messed up as well and there's a lot of asymmetry and then if we look here I suspect it's all going to
1:45:29
be also pretty messed up so you see there's a lot of discrepancy in how fast
1:45:34
these layers are learning and some of them are learning way too fast so negative one negative 1.5 those aren't
1:45:41
very large numbers in terms of this ratio again you should be somewhere around negative three and not much more
1:45:46
about that so this is how miscalibration so if your neural nuts are going to
1:45:52
manifest and these kinds of plots here are a good way of um sort of bringing those
1:45:58
miscalibrations sort of uh to your attention and so you can address them okay so so far we've seen that when
bringing back batchnorm, looking at the visualizations
1:46:06
we have this linear 10h sandwich we can actually precisely calibrate the gains and make the activations the gradients
1:46:12
and the parameters and the updates all look pretty decent but it definitely feels a little bit like balancing
1:46:18
of a pencil on your finger and that's because this gain has to be very precisely calibrated
1:46:25
so now let's introduce batch normalization layers into the fix into the mix and let's let's see how that
1:46:31
helps fix the problem so here I'm going to take the bachelor Monday
1:46:37
class and I'm going to start placing it inside and as I mentioned before the standard
1:46:43
typical place you would place it is between the linear layer so right after it but before the nonlinearity but
1:46:49
people have definitely played with that and uh in fact you can get very similar results even if you place it after the
1:46:55
nonlinearity and the other thing that I wanted to mention is it's totally fine to also place it at the end after the last
1:47:02
linear layer and before the loss function so this is potentially fine as well
1:47:08
and in this case this would be output would be world cup size
1:47:14
um now because the last layer is mushroom we would not be changing the weight to make the softmax less
1:47:19
confident we'd be changing the gamma because gamma remember in the bathroom is the variable that multiplicatively
1:47:27
interacts with the output of that normalization so we can initialize this sandwich now
1:47:35
and we can train and we can see that the activations are going to of course look
1:47:40
very good and they are going to necessarily look good because now before every single 10 H layer there is a
1:47:47
normalization in The Bachelor so this is unsurprisingly all looks pretty good
1:47:52
it's going to be standard deviation of roughly 0.65 two percent and roughly equal standard deviation throughout the
1:47:59
entire layers so everything looks very homogeneous the gradients look good the weights look
1:48:06
good and they're distributions and then the updates
1:48:11
also look pretty reasonable we're going above negative three a little bit but
1:48:16
not by too much so all the parameters are training in roughly the same rate here
1:48:24
but now what we've gained is we are going to be slightly less um
1:48:31
brittle with respect to the gain of these so for example I can make the gain be say 0.2 here
1:48:38
um which was much much slower than what we had with the 10h but as we'll see the activations will
1:48:44
actually be exactly unaffected and that's because of again this explicit normalization the gradients are going to
1:48:50
look okay the weight gradients are going to look okay but actually the updates will change
1:48:56
and so even though the forward and backward pass to a very large extent look okay because of the backward pass of the
1:49:03
batch form and how the scale of the incoming activations interacts in the basharm and its backward pass this is
1:49:10
actually changing the um the scale of the updates on these parameters so the grades and ingredients
1:49:17
of these weights are affected so we still don't get a completely free pass to pass an arbitrary weights here
1:49:24
but it everything else is significantly more robust in terms of the forward
1:49:30
backward and the weight gradients it's just that you may have to retune your learning rate if you are changing
1:49:36
sufficiently the the scale of the activations that are coming into the bachelor's so here for example this we
1:49:43
changed the gains of these linear layers to be greater and we're seeing that the updates are coming out lower as a result
1:49:51
and then finally we can also if we are using basharms we don't actually need to necessarily let me reset this to one so
1:49:58
there's no gain we don't necessarily even have to um normalize by fan in sometimes so if I
1:50:04
take out the fan in so these are just now uh random gaussian we'll see that because of batch drum
1:50:09
this will actually be relatively well behaved so
1:50:14
this this is look of course in the forward pass look good the gradients look good the backward the weight updates look
1:50:22
okay A little bit of fat tails in some of the layers and this looks okay as well but as you
1:50:29
as you can see uh we're significantly below negative three so we'd have to bump up the learning rate of this
1:50:35
bachelor so that we are training more properly and in particular looking at this roughly looks like we have to 10x
1:50:42
the learning rate to get to about 20 negative three so we come here and we would change this
1:50:48
to be update of 1.0 and if when I reinitialize
1:50:59
then we'll see that everything still of course looks good and now we are roughly here and we
1:51:04
expect this to be an okay training run so long story short we are significantly more robust to the gain of these linear
1:51:11
layers whether or not we have to apply the fan in and then we can change the gain but we actually do have to worry a
1:51:18
little bit about the update um scales and making sure that the learning rate is properly calibrated
1:51:23
here but thus the activations of the forward backward pass and the updates are all are looking significantly more
1:51:29
well-behaved except for the global scale that is potentially being adjusted here okay so now let me summarize there are
summary of the lecture for real this time
1:51:37
three things I was hoping to achieve with this section number one I wanted to introduce you to batch normalization
1:51:42
which is one of the first modern innovations that we're looking into that helped stabilize very deep neural
1:51:48
networks and their training and I hope you understand how the bachelorization works and how it would be used in a
1:51:54
neural network number two I was hoping to pie tortify some of our code and wrap it up into
1:52:00
these modules so like linear Bachelor 1D 10h Etc these are layers or modules and
1:52:07
they can be stacked up into neural Nets like Lego building blocks and these
1:52:12
layers actually exist in pie torch and if you import torch and then you can
1:52:17
actually the way I've constructed it you can simply just use pytorch by prepending and then dot to all these
1:52:23
different layers and actually everything will just work because the API that I've developed here
1:52:29
is identical to the API that pytorch uses and the implementation also is basically as far as I'm aware identical
1:52:36
to the one in pi torch and number three I try to introduce you to the diagnostic tools that you would use to understand whether your neural
1:52:43
network is in a good State dynamically so we are looking at the statistics and histograms and activation of the forward
1:52:50
pass application activations the backward pass gradients and then also we're looking at the weights that are
1:52:56
going to be updated as part of stochastic already in ascent and we're looking at their means standard deviations and also the ratio of
1:53:03
gradients to data or even better the updates to data and we saw that
1:53:09
typically we don't actually look at it as a single snapshot Frozen in time at some particular iteration typically
1:53:14
people look at this as uh over time just like I've done here and they look at these updated data ratios and they make
1:53:20
sure everything looks okay and in particular I said that um running negative 3 or basically
1:53:26
negative 3 on the log scale is a good rough heuristic for what you want this ratio to be and if it's way too high
1:53:32
then probably the learning rate or the updates are a little too too big and if it's way too small that the learning
1:53:37
rate is probably too small so that's just some of the things that you may want to play with when you try
1:53:43
to get your neural network to work with very well now there's a number of things I did not
1:53:48
try to achieve I did not try to beat our previous performance as an example by introducing the bathroom layer actually
1:53:54
I did try um and I found the new I used the learning rate finding mechanism that I've described before I tried to train
1:54:00
the bathroom layer a bachelor neural nut and I actually ended up with results that are very very similar to what we've
1:54:06
obtained before and that's because our performance now is not bottlenecked by the optimization
1:54:12
which is what bass Norm is helping with the performance at this stage is bottlenecked by what I suspect is the
1:54:18
context length of our context So currently we are taking three characters to predict the fourth one and
1:54:25
I think we need to go beyond that and we need to look at more powerful architectures that are like recurrent neural networks and Transformers in
1:54:31
order to further push um the log probabilities that we're achieving on this data set
1:54:36
and I also did not try to have a full explanation of all of these activations
1:54:41
the gradients and the backward paths and the statistics of all these gradients and so you may have found some of the
1:54:46
parts here unintuitive and maybe you're slightly confused about okay if I change the gain here how come that we need a
1:54:53
different learning rate and I didn't go into the full detail because you'd have to actually look at the backward pass of all these different layers and get an
1:54:59
intuitive understanding of how that works and I did not go into that in this lecture the purpose really was just to
1:55:05
introduce you to the diagnostic tools and what they look like but there's still a lot of work remaining on the intuitive level to understand the
1:55:12
initialization the backward pass and how all of that interacts but you shouldn't feel too bad because honestly we are
1:55:18
getting to The Cutting Edge of where the field is we certainly haven't I would
1:55:24
say solved initialization and we haven't solved back propagation and these are still very much an active area of
1:55:30
research people are still trying to figure out what's the best way to initialize these networks what is the best update rule to use
1:55:36
um and so on so none of this is really solved and we don't really have all the answers to all the uh to you know all
1:55:43
these cases but at least you know we're making progress at least we have some tools to tell us whether or not things
1:55:48
are on the right track for now so I think we've made positive progress in
1:55:54
this lecture and I hope you enjoyed that and I will see you next time